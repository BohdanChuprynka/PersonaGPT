{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is for analyzing the steps during processing data. It contains a lot more documentation and code than the original script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "# Processing Optimization \n",
    "from functools import cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"Datasets/stopwords_ua_set.txt\"):\n",
    "      !wget -P\"Datasets/\" https://raw.githubusercontent.com/skupriienko/Ukrainian-Stopwords/refs/heads/master/stopwords_ua_set.txt\n",
    "\n",
    "with open('Datasets/stopwords_ua_set.txt', 'r') as file:\n",
    "    ukrainian_stop_words = file.read().splitlines()[0]\n",
    "\n",
    "keys_to_filter = os.getenv('KEYS_TO_FILTER').split(',')\n",
    "concatenated_path = os.getenv('CONCATENATED_PATH')\n",
    "dataset_path = \"Datasets/concatenated.csv\"\n",
    "english_topwords = set(stopwords.words('english'))\n",
    "\n",
    "# word2vec_path = \"Models/glove-global-vectors-for-word-representation.zip\" \n",
    "# glove_100d_path = \"Models/glove.6B.100d.txt\"\n",
    "# if not os.path.exists(word2vec_path) or not os.path.exists(glove_100d_path):\n",
    "#     print(\"Downloading word2vec for data augmentation...\")\n",
    "#     !kaggle datasets download -d rtatman/glove-global-vectors-for-word-representation -p Models/\n",
    "#     !unzip Models/glove-global-vectors-for-word-representation.zip -d Models\n",
    "#     os.remove(\"Models/glove-global-vectors-for-word-representation.zip\")\n",
    "#     os.remove(\"Models/glove.6B.200d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(dataset_path)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "      return re.sub(r'http\\S+', 'redacted', text)\n",
    "# For non-english datasets\n",
    "def remove_english_words(text):\n",
    "    # Looks for all English words and removes them.\n",
    "    pattern = r'\\b[a-zA-Z]+\\b'\n",
    "    return re.sub(pattern, '', text)\n",
    "def delete_html_tags(text):\n",
    "    clean_text = re.sub(r'<.*?>', '', text)\n",
    "    return clean_text\n",
    "def remove_mention(text):\n",
    "  mention_regex = r\"@\\w+\"\n",
    "  return re.sub(mention_regex, \"/mention\", text)\n",
    "def redact_email(text): \n",
    "    return re.sub(r'\\S+@\\S+', '/email', text)\n",
    "# def remove_password(text): \n",
    "#     copy_text = text\n",
    "#     pass_pattern = r'[A-Za-z0-9@#$%^&+=]{8,}'\n",
    "#     text_ = re.sub(pass_pattern, '', text)\n",
    "#     return text_\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "def sen_len_threshold(text, char_min=16, char_limit=512): # Can be used for better tuning. \n",
    "    text = str(text)\n",
    "    # Removes sentences if between char_min and char_limit.\n",
    "    clean_text = text if char_min <= len(text) <= char_limit else None\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, ' ', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sensitive_words(sentence, replacement='CENSORED', keys_to_filter=keys_to_filter):\n",
    "    \"\"\"\n",
    "    Create a list of sensitive words 'keys_to_filter' from .env file \n",
    "    Replaces sensitive for you words with 'CENSORED'\n",
    "\n",
    "    Parameters: \n",
    "        sentence \n",
    "        replacement: str = words that will be substituted instead of the sensitive words   \n",
    "    \"\"\"\n",
    "    words = set(keys_to_filter)\n",
    "    sentence_words = word_tokenize(sentence)\n",
    "    \n",
    "    modified_sentence = [\n",
    "        replacement if word in words else word for word in sentence_words\n",
    "    ]\n",
    "    \n",
    "    # Join the list back into a sentence\n",
    "    return ' '.join(modified_sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since \" \" rows don't count as NAN, we should identify them by ourselves.\n",
    "def drop_space_rows(df: pd.DataFrame, column: str =\"Message\") -> pd.DataFrame:\n",
    "      \"\"\"Identifies and drops ' ' rows in the DataFrame\"\"\"\n",
    "      space_rows = (df[column] == ' ')| (df[column] == '')\n",
    "      df_filtered = df[~pd.Series(space_rows)].reset_index(drop=True)\n",
    "\n",
    "      return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "      text = remove_english_words(text)\n",
    "      text = redact_email(text)\n",
    "      text = remove_urls(text)\n",
    "      text = remove_mention(text)\n",
    "      text = delete_html_tags(text)\n",
    "      text = filter_sensitive_words(text)\n",
    "      text = remove_whitespace(text)\n",
    "      \n",
    "      return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    import time \n",
    "    dataset_copy = df.copy()\n",
    "    start_time= time.time()\n",
    "    df['Message'] = df['Message'].apply(preprocess_data)\n",
    "    df[\"Message\"] = df[\"Message\"].apply(lambda x: remove_emojis(str(x)) if isinstance(x, str) else ' ')\n",
    "    df = drop_space_rows(df)\n",
    "    print(df.head(10))\n",
    "    df.to_csv(concatenated_path, index=False)\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total time for processing: {total_time:.2f} seconds\")\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copy = dataset.copy() # For visual purposes\n",
    "dataset = preprocess_dataset(dataset)\n",
    "\n",
    "b_length = len(dataset_copy)\n",
    "a_length = len(dataset)\n",
    "b_mean_length = np.mean(dataset_copy['Message'].str.len())\n",
    "a_mean_length = np.mean(dataset['Message'].str.len())\n",
    "b_max_length = np.max(dataset_copy['Message'].str.len())\n",
    "a_max_length = np.max(dataset['Message'].str.len())\n",
    "longest_sentence_index = dataset['Message'].str.len().idxmax()\n",
    "longest_sentence = dataset['Message'].iloc[longest_sentence_index]\n",
    "\n",
    "\n",
    "print(f\"Changes (Before/After) processing:\")\n",
    "print(f\"Length: {b_length} -> {a_length}\")\n",
    "print(f\"Median length: {b_mean_length:.2f} -> {a_mean_length:.2f}\")\n",
    "print(f\"Max sentence length: {b_max_length} -> {a_max_length}\")\n",
    "print(f\"Nan values: {dataset_copy.isna().sum().sum()} -> {dataset.isna().sum().sum()}\")\n",
    "print(f\"Longest sentence: {len(longest_sentence)} chars: {longest_sentence}\")\n",
    "\n",
    "del dataset_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving into Question / Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating a column with time difference between messages \n",
    "To correctly assign the context.\n",
    "\"\"\"\n",
    "dataset = dataset.sort_values(by=['Date']).reset_index(drop=True)\n",
    "\n",
    "dataset['Date'] = pd.to_datetime(dataset['Date'], format='ISO8601')\n",
    "\n",
    "reference_time = dataset['Date'].min()\n",
    "dataset['time_diff_seconds'] = dataset['Date'] - reference_time\n",
    "# Converts into hours difference\n",
    "dataset['time_diff_seconds'] = dataset['time_diff_seconds'].apply(lambda x: int(x.total_seconds()))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sentences(df: pd.DataFrame) -> pd.DataFrame:\n",
    "      \"\"\"\n",
    "      Takes a pandas dataframe with a messages column and returns separated rows with question / answer columns\n",
    "      Args: \n",
    "            dataset: pd.DataFrame\n",
    "            Dataset should contain a messages column and first row with identification who sent a message.\n",
    "\n",
    "\n",
    "\n",
    "      Returns:\n",
    "            dataset: pd.DataFrame\n",
    "            \n",
    "            Dataset divided into question / answer columns.\n",
    "      \"\"\"\n",
    "\n",
    "      separated_dataset = pd.DataFrame(columns=['question', 'answer', 'timestamp', 'Sent_by_me', 'time_diff_seconds'])\n",
    "\n",
    "      # Make the first row the first question (All questions become even, answers->odds)\n",
    "      if df[\"Sent_by_me\"].iloc[0]: \n",
    "            df = df.drop(df.index[0]).reset_index(drop=True)\n",
    "\n",
    "      questions_df = df[df.index % 2 == 0].reset_index(drop=True)\n",
    "      answers_df = df[df.index % 2 == 1].reset_index(drop=True)\n",
    "\n",
    "      min_length = min(len(questions_df), len(answers_df))\n",
    "\n",
    "      separated_dataset = pd.concat(\n",
    "     [\n",
    "        questions_df[\"Message\"][:min_length].rename(\"question\"),\n",
    "        answers_df[\"Message\"][:min_length].rename(\"answer\"),\n",
    "        df[\"Date\"][:min_length].rename(\"timestamp\"),\n",
    "        df[\"Sent_by_me\"][:min_length].rename(\"Sent_by_me\"),\n",
    "        df[\"time_diff_seconds\"][:min_length].rename(\"time_diff_seconds\")\n",
    "     ], axis=1\n",
    ")\n",
    "\n",
    "      return separated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = separate_sentences(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding column with previous context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_context(df: pd.DataFrame, context_size: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a column with previous context to the DataFrame.\n",
    "    \n",
    "    The context is based on the previous messages. If the time difference \n",
    "    between messages is more than 2 hours, it's considered the start of a \n",
    "    new conversation, and the first row of that new conversation will have \n",
    "    no context. Subsequent messages in the conversation will have context.\n",
    "    \"\"\"\n",
    "    \n",
    "    context_list = []\n",
    "    last_time = None  # Track the last message time to determine time gaps\n",
    "    \n",
    "    for index in range(len(df)):\n",
    "        if index == 0:\n",
    "            # No context for the very first message\n",
    "            context_list.append(None)\n",
    "            last_time = df.loc[index, \"time_diff_seconds\"]\n",
    "            continue\n",
    "        \n",
    "        # Calculate the time difference from the previous row\n",
    "        time_diff = df.loc[index, \"time_diff_seconds\"] - last_time\n",
    "        last_time = df.loc[index, \"time_diff_seconds\"]\n",
    "\n",
    "        # If time_diff is more than 6 hours, consider it a new conversation\n",
    "        if time_diff > 21600:\n",
    "            context_list.append(None)  # Start of a new conversation, no context\n",
    "        else:\n",
    "            # Create context from the previous messages within the context size\n",
    "            start_index = max(index - context_size, 0)\n",
    "            context = df.loc[start_index:index - 1, [\"question\", \"answer\"]]\n",
    "\n",
    "            # Build the context string from previous rows\n",
    "            message = []\n",
    "            for key, (question, answer) in enumerate(zip(context[\"question\"], context[\"answer\"])):\n",
    "                message.append(f\"Q{key + 1}: {question}. A{key + 1}: {answer} || \")\n",
    "\n",
    "            # Append the concatenated message as the context\n",
    "            context_list.append(\" \".join(message))\n",
    "\n",
    "    # Handle 1st row None (diff seconds in 0 index is 0, then 1 is None).\n",
    "    context = df.loc[0, [\"question\", \"answer\"]]\n",
    "    question, answer = context[\"question\"], context[\"answer\"]\n",
    "    context_list[1] = (f\"Q{1}: {question}. A{1}: {answer} || \")\n",
    "    \n",
    "    # Add the context as a new column\n",
    "    df[\"context\"] = context_list\n",
    "\n",
    "    # Replace any empty or missing contexts with \"Missing Context\" if desired\n",
    "    df[\"context\"] = df[\"context\"].apply(lambda x: \"Time Gap\" if pd.isna(x) else x)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = add_context(dataset)\n",
    "dataset.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time_gaps = (dataset[\"context\"] == \"Time Gap\").sum()\n",
    "total_time_gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation \n",
    "and continue of processing\n",
    "\n",
    "Resources: \n",
    "https://arxiv.org/pdf/1901.11196\n",
    "\n",
    "Methods: \n",
    "1. Back-translation\n",
    "2. Synonym replacement\n",
    "3. Word Swap\n",
    "4. Sentence shuffle\n",
    "\n",
    "Remember that this notebook is designed to work with ukrainian language dataset, and not all techniques will work for English language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def remove_double_commas(text: str) -> str:\n",
    "    \"\"\"Removes double commas from the text.\"\"\"\n",
    "    return text.replace(\",,\", \",\")\n",
    "\n",
    "def split_sentences(text: str) -> list:\n",
    "    \"\"\"Splits the text into sentences by commas, handling empty strings gracefully.\"\"\"\n",
    "    return [sentence.strip() for sentence in text.split(',') if sentence.strip()]\n",
    "\n",
    "def shuffle_sentence(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes double commas, splits the text into sentences, shuffles them,\n",
    "    and joins them back into a shuffled sentence.\n",
    "    \"\"\"\n",
    "    # Step 1: Clean and split the sentences\n",
    "    clean_text = remove_double_commas(text)\n",
    "    sentences = split_sentences(clean_text)\n",
    "\n",
    "    # Step 2: Shuffle the sentences\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "    # Step 3: Join shuffled sentences back into a single string\n",
    "    return \", \".join(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> for shuffle example.\n",
    "\n",
    "<!--\n",
    "text = \"This is a test,, sentence, another part,, and more text. Це просто тест, такий вот тест\"\n",
    "shuffled_text = shuffle_sentence(text)\n",
    "\n",
    "print(f\"Before: {text}\")\n",
    "print(f\"After: {shuffled_text}\")\n",
    "\n",
    "Outputs: \n",
    "Before: This is a test,, sentence, another part,, and more text. Це просто тест, такий вот тест\n",
    "After: and more text. Це просто тест, another part, This is a test, sentence, такий вот тест\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-translation using MarianMTModel\n",
    "**Not unilizing in the project because of the slow generation time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> to see MarianMTModel\n",
    "\n",
    "\n",
    "<!--\n",
    "# Helper function to download data for a language\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def download(model_name):\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "# download model for English -> Ukrainian\n",
    "first_tokenizer, first_model = download('Helsinki-NLP/opus-mt-uk-en')\n",
    "# download model for Ukrainian -> English\n",
    "second_tokenizer, second_model = download('Helsinki-NLP/opus-mt-en-uk')\n",
    "\n",
    "def format_batch_texts(language_code, batch_text):\n",
    "    formated_batch = [f\">>{language_code}<< {batch_text}\"]\n",
    "\n",
    "    return formated_batch\n",
    "\n",
    "def translate(batch_texts, model, tokenizer, language):\n",
    "    \"\"\"Translate texts into a target language\"\"\"\n",
    "    # Format the text as expected by the model\n",
    "    batched_text = format_batch_texts(language, batch_texts)\n",
    "\n",
    "    # Translate\n",
    "    translated = [model.generate(**tokenizer(batch_texts, return_tensors=\"pt\", padding=True)) for sentence in batched_text]\n",
    "\n",
    "    # Decode (tokens to text)\n",
    "    translated_texts = tokenizer.batch_decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_texts\n",
    "\n",
    "def back_translate(texts, from_language=\"uk\", to_language = \"en\"):\n",
    "    \"\"\"Implements back translation\"\"\"\n",
    "    # Translate from source to target language\n",
    "    if from_language == \"uk\": \n",
    "        translated = translate(texts, first_model, first_tokenizer, from_language)\n",
    "        back_translated = translate(translated, second_model, second_tokenizer, to_language)[0]\n",
    "        return back_translated\n",
    "    \n",
    "    translated = translate(texts, second_model, second_tokenizer, from_language)\n",
    "    back_translated = translate(translated, first_model, first_tokenizer, to_language)[0]\n",
    "\n",
    "    return back_translated\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> for back-translation example with MarianMTModel\n",
    "\n",
    "<!--\n",
    "# Perform back-translation (Ukrainian to English to Ukrainian)\n",
    "texts = [\"Це перше речення яке ти маєш перекласти.\",\n",
    "         \"Воно є дуже просте та правильно сформульованею.\"]\n",
    "back_translated_texts = back_translate(texts)\n",
    "texts = [\"This is the first sentence you should translate\", \n",
    "        \"It is simple and correctly formulated\"]\n",
    "back_translated_texts_en = back_translate(texts, \"en\", \"uk\")\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Text:\", texts)\n",
    "print(\"Back-Translated Text:\", back_translated_texts)\n",
    "print(\"-----------------\")\n",
    "print(\"Original Text:\", texts)\n",
    "print(\"Back-Translated Text:\", back_translated_texts_en)\n",
    "\n",
    "Outputs:\n",
    "Original Text: ['This is the first sentence you should translate', 'It is simple and correctly formulated']\n",
    "Back-Translated Text: ['Це перше речення, яке ви маєте перекласти.', 'Вона дуже проста і добре сформульована.']\n",
    "-----------------\n",
    "Original Text: ['This is the first sentence you should translate', 'It is simple and correctly formulated']\n",
    "Back-Translated Text: ['This is the first sentence you have to translate.', \"It's simple and correctly formulated.\"]\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swap and Pop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_word(sentence): \n",
    "    \"\"\"Swaps two random words in the sentence\"\"\"\n",
    "    words = word_tokenize(sentence)\n",
    "    if len(words) < 2:\n",
    "        return sentence\n",
    "\n",
    "    idx1, idx2 = np.random.choice(len(words), size=2, replace=False)\n",
    "    words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(sentence, stop_words=ukrainian_stop_words) -> str:\n",
    "    \"\"\" Returns two lists: words with stopwords and words without stopwords\"\"\"\n",
    "    words = word_tokenize(sentence)\n",
    "    filtered_stopwords = [word for word in words if word.lower() not in stop_words]\n",
    "    return words, filtered_stopwords\n",
    "\n",
    "def pop_word(sentence, word_swap: bool = False):\n",
    "    \"\"\"Pops a random word from the sentence\"\"\"\n",
    "\n",
    "    words, stop_words = filter_stopwords(sentence)\n",
    "\n",
    "    if stop_words: \n",
    "        remove_index = np.random.choice(stop_words, size=1, replace=False)[0]\n",
    "        words.remove(remove_index)\n",
    "    else: \n",
    "        return sentence\n",
    "\n",
    "\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> for word elimination example.\n",
    "\n",
    "<!--\n",
    "# Example: \n",
    "\n",
    "example = \"Це є експериментальним реченням. Воно прикольне))\" # Stopwords work only with ukrainian language.\n",
    "example = pop_word(example)\n",
    "\n",
    "print(example)\n",
    "\n",
    "Outputs: \n",
    "\"is a random sentence\"\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> for word swap example.\n",
    "\n",
    "<!--\n",
    "# Example: \n",
    "\n",
    "example = \"This is a random sentence\"\n",
    "example = swap_word(example)\n",
    "\n",
    "print(example)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back translation and Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_CODES = {\n",
    "    'afrikaans': 'af',\n",
    "    'albanian': 'sq',\n",
    "    'amharic': 'am',\n",
    "    'arabic': 'ar',\n",
    "    'armenian': 'hy',\n",
    "    'azerbaijani': 'az',\n",
    "    'basque': 'eu',\n",
    "    'belarusian': 'be',\n",
    "    'bengali': 'bn',\n",
    "    'bosnian': 'bs',\n",
    "    'bulgarian': 'bg',\n",
    "    'catalan': 'ca',\n",
    "    'cebuano': 'ceb',\n",
    "    'chichewa': 'ny',\n",
    "    'chinese (simplified)': 'zh-cn',\n",
    "    'chinese (traditional)': 'zh-tw',\n",
    "    'corsican': 'co',\n",
    "    'croatian': 'hr',\n",
    "    'czech': 'cs',\n",
    "    'danish': 'da',\n",
    "    'dutch': 'nl',\n",
    "    'english': 'en',\n",
    "    'esperanto': 'eo',\n",
    "    'estonian': 'et',\n",
    "    'filipino': 'tl',\n",
    "    'finnish': 'fi',\n",
    "    'french': 'fr',\n",
    "    'frisian': 'fy',\n",
    "    'galician': 'gl',\n",
    "    'georgian': 'ka',\n",
    "    'german': 'de',\n",
    "    'greek': 'el',\n",
    "    'gujarati': 'gu',\n",
    "    'haitian creole': 'ht',\n",
    "    'hausa': 'ha',\n",
    "    'hawaiian': 'haw',\n",
    "    'hebrew': 'he',\n",
    "    'hindi': 'hi',\n",
    "    'hmong': 'hmn',\n",
    "    'hungarian': 'hu',\n",
    "    'icelandic': 'is',\n",
    "    'igbo': 'ig',\n",
    "    'indonesian': 'id',\n",
    "    'irish': 'ga',\n",
    "    'italian': 'it',\n",
    "    'japanese': 'ja',\n",
    "    'javanese': 'jw',\n",
    "    'kannada': 'kn',\n",
    "    'kazakh': 'kk',\n",
    "    'khmer': 'km',\n",
    "    'korean': 'ko',\n",
    "    'kurdish (kurmanji)': 'ku',\n",
    "    'kyrgyz': 'ky',\n",
    "    'lao': 'lo',\n",
    "    'latin': 'la',\n",
    "    'latvian': 'lv',\n",
    "    'lithuanian': 'lt',\n",
    "    'luxembourgish': 'lb',\n",
    "    'macedonian': 'mk',\n",
    "    'malagasy': 'mg',\n",
    "    'malay': 'ms',\n",
    "    'malayalam': 'ml',\n",
    "    'maltese': 'mt',\n",
    "    'maori': 'mi',\n",
    "    'marathi': 'mr',\n",
    "    'mongolian': 'mn',\n",
    "    'myanmar (burmese)': 'my',\n",
    "    'nepali': 'ne',\n",
    "    'norwegian': 'no',\n",
    "    'odia': 'or',\n",
    "    'pashto': 'ps',\n",
    "    'persian': 'fa',\n",
    "    'polish': 'pl',\n",
    "    'portuguese': 'pt',\n",
    "    'punjabi': 'pa',\n",
    "    'romanian': 'ro',\n",
    "    'russian': 'ru',\n",
    "    'samoan': 'sm',\n",
    "    'scots gaelic': 'gd',\n",
    "    'serbian': 'sr',\n",
    "    'sesotho': 'st',\n",
    "    'shona': 'sn',\n",
    "    'sindhi': 'sd',\n",
    "    'sinhala': 'si',\n",
    "    'slovak': 'sk',\n",
    "    'slovenian': 'sl',\n",
    "    'somali': 'so',\n",
    "    'spanish': 'es',\n",
    "    'sundanese': 'su',\n",
    "    'swahili': 'sw',\n",
    "    'swedish': 'sv',\n",
    "    'tajik': 'tg',\n",
    "    'tamil': 'ta',\n",
    "    'telugu': 'te',\n",
    "    'thai': 'th',\n",
    "    'turkish': 'tr',\n",
    "    'ukrainian': 'uk',\n",
    "    'urdu': 'ur',\n",
    "    'uyghur': 'ug',\n",
    "    'uzbek': 'uz',\n",
    "    'vietnamese': 'vi',\n",
    "    'welsh': 'cy',\n",
    "    'xhosa': 'xh',\n",
    "    'yiddish': 'yi',\n",
    "    'yoruba': 'yo',\n",
    "    'zulu': 'zu'}\n",
    "\n",
    "LANGUAGES = {value:key for key, value in LANG_CODES.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as naf\n",
    "import psutil\n",
    "\n",
    "from nlpaug.util import Action\n",
    "\n",
    "aug_glove = naw.WordEmbsAug(\n",
    "    model_type='glove', model_path='Models/glove.6B.100d.txt',\n",
    "    action=\"substitute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "class google_translate:\n",
    "    \"\"\"\n",
    "    Performs Google Translate on a given text.\n",
    "\n",
    "    Args:\n",
    "        translate_from (str): The natural language of the text. Defaults to \"uk\". Contains auto language detection.\n",
    "        translate_to (str): The language to translate to and back from. Defaults to \"en\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, translate_from: str = \"uk\", translate_to: str = \"en\", replace_synonyms: bool = False):\n",
    "        self.native_language = translate_from\n",
    "        self.tunnel_language = translate_to \n",
    "        self.translator = Translator()\n",
    "\n",
    "        if replace_synonyms:\n",
    "            self.word2vec_model = self.install_word2vec()\n",
    "\n",
    "    \"\"\" Back-translation \"\"\"\n",
    "    # Check whether the language input is correct\n",
    "    @cache\n",
    "    def check_language(self, text):\n",
    "        try: \n",
    "            if self.native_language not in LANGUAGES:  \n",
    "                self.native_language = self.translator.detect(text).lang\n",
    "                print(f\"Incorrect language. Translating from '{self.native_language}'\")\n",
    "\n",
    "                # If the back-translation is going on English text, the text will be translated from English to Spanish and back to English.\n",
    "                if self.native_language == \"en\": \n",
    "                    self.tunnel_language = \"es\"\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Check_language: \" + str(e))\n",
    "\n",
    "    @cache \n",
    "    def back_translate(self, text, replace_synonym: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Performs back-translation on a given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to back-translate.\n",
    "            temp_lang (str): The intermediate language for translation. Defaults to French (\"fr\").\n",
    "\n",
    "        Returns:\n",
    "            str: The back-translated text.\n",
    "        \"\"\"\n",
    "        translator = self.translator\n",
    "        try: \n",
    "            self.check_language(text=text)\n",
    "\n",
    "            translated = self.translator.translate(text, src=self.native_language, dest=self.tunnel_language).text\n",
    "            \n",
    "            if replace_synonym: \n",
    "                translated = self.synonym_replacement(sentence=translated) \n",
    "\n",
    "            back_translated = translator.translate(translated, src=self.tunnel_language, dest=self.native_language).text\n",
    "\n",
    "            return back_translated\n",
    "        except Exception as e: \n",
    "            print(\"back_translate: Something went wrong.\")\n",
    "\n",
    "    \"\"\" Synonym extension (Word2Vec) \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    def install_word2vec(self):\n",
    "      model_name = \"word2vec-google-news-300\"\n",
    "      print(f\"Configuring {model_name}\")\n",
    "      word2vec_model = api.load(model_name)\n",
    "\n",
    "      return word2vec_model\n",
    "\n",
    "    @cache\n",
    "    def synonym_replacement(self, sentence, percentage: float = 0.7): \n",
    "        \"\"\" Replaces random non-stopword word with a synonym. \n",
    "\n",
    "        Args:\n",
    "            percentage (float, optional): Percentage of words to replace. Defaults to 0.7.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Remove stopwords \n",
    "        words, filtered_sentence = filter_stopwords(sentence)\n",
    "        if words: \n",
    "            try: \n",
    "                random_word_index = np.random.choice(len(filtered_sentence), size=int(percentage * len(filtered_sentence) if len(filtered_sentence) > 1 else 1))[0]\n",
    "                word_to_replace = filtered_sentence[random_word_index]\n",
    "                synonym = self.word2vec_model.most_similar(word_to_replace, topn=1)[0][0] # Top 5 most similar words\n",
    "                # Fill the chosen word for a synomym\n",
    "                for idx, word in enumerate(words): \n",
    "                    if word == word_to_replace: \n",
    "                        words[idx] = word_to_replace\n",
    "\n",
    "                return \" \".join(words)\n",
    "            except Exception as e: \n",
    "                print(\"synonym_replacement Exception: Could not replace synonym.\")\n",
    "                return sentence        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> to see back-translation example.\n",
    "\n",
    "\n",
    "<!--\n",
    "translator = google_translate(translate_from=\"uk\", translate_to=\"en\")\n",
    "back_translated = translator.back_translate(\"Привіт, як воно?\") # Hello, how is it going?\n",
    "back_translated\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating augmentation rows and concatenating them with dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> for short augmentation example \n",
    "\n",
    "<!--\n",
    "translator = google_translate(translate_from=\"uk\", translate_to=\"en\", replace_synonyms=True)\n",
    "\n",
    "# Short implementation of random augmentation\n",
    "augmentation_functions = [translator.back_translate, shuffle_sentence, pop_word, swap_word]\n",
    "indexes = np.random.choice(len(augmentation_functions), size=random.randint(1, 4), replace=False)   \n",
    "functions = [augmentation_functions[index] for index in sorted(indexes)]\n",
    "\n",
    "sentence = \"Example\"\n",
    "print(functions)\n",
    "for function in functions: \n",
    "    sentence = function(sentence)\n",
    "sentence\n",
    "---->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = google_translate(translate_from=\"uk\", translate_to=\"en\", replace_synonyms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_memory(threshold_gb: float = 4, delay: int = 10): \n",
    "    \"\"\"\n",
    "    Pauses execution when available memory is less than threshold.\n",
    "    Args:f\n",
    "    - threshold_gb (float): Max memory allowed in GB.\n",
    "    - delay (int): Seconds to wait before rechecking memory.\n",
    "    \"\"\"\n",
    "    available_ram = psutil.virtual_memory().available / (1024**3)\n",
    "    if available_ram <= threshold_gb:\n",
    "        print(available_ram)\n",
    "        #print(\"Memory limit reached. Waiting for resources to free up...\")\n",
    "        time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_functions = [translator.back_translate, shuffle_sentence, pop_word, swap_word]\n",
    "def select_random_functions(functions=augmentation_functions, p=[0.1, 0.3, 0.3, 0.3]):  # Lowered probabilities for back-translation because of low-resources\n",
    "    \"\"\" Returns random functions in order to apply during processing\"\"\"\n",
    "\n",
    "    indexes = sorted(np.random.choice(len(functions), size=random.randint(1, len(functions)), replace=False, p=p))\n",
    "    return [functions[index] for index in indexes]            \n",
    "\n",
    "def apply_augmentation(sentence, random_augmentation: bool = True) -> pd.DataFrame:\n",
    "    try: \n",
    "        # Check for available memory \n",
    "        is_memory()\n",
    "\n",
    "        if random_augmentation:\n",
    "            functions = select_random_functions()\n",
    "            for function in functions:\n",
    "                sentence = function(sentence)\n",
    "            return sentence \n",
    "        \n",
    "        sentence = translator.back_translate(sentence, replace_synonym=True)\n",
    "        sentence = shuffle_sentence(sentence)\n",
    "        sentence = swap_word(sentence)\n",
    "        sentence = pop_word(sentence)\n",
    "    except Exception as e: \n",
    "        print(\"apply_augmentation EXCEPTION: \" + str(e))\n",
    "        return sentence\n",
    "\n",
    "def speed_test(df, samples: int = 100) -> None:\n",
    "    start_time = time.time()\n",
    "    df[\"question\"] = df[\"question\"][:samples].apply(lambda x: apply_augmentation(x))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return \n",
    "\n",
    "def augment_data(df: pd.DataFrame, \n",
    "                save_path: str = None,\n",
    "                augmentation_factor: int = 5, \n",
    "                random_augmentation: bool = False, \n",
    "                samples: int = None, \n",
    "                checkpoints: bool = True) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Augments the data by adding augmented questions.\n",
    "    \n",
    "    Parameters:\n",
    "        df: pd.DataFrame with \"question\" column\n",
    "        augmentation_factor: int = 5; how many times to augment each question.\n",
    "        samples: int = None; How much rows to process. \n",
    "        checkpoints: bool = True; Saves the augmentation process every iteration (augmentation_factor==1Iter)\n",
    "    \"\"\"\n",
    "    original_dataframe = df[:200]\n",
    " \n",
    "\n",
    "    df_augmented = original_dataframe.copy()\n",
    "    df_augmented = drop_space_rows(df_augmented, column=\"question\")\n",
    "    for _ in tqdm(range(augmentation_factor)):\n",
    "        loop_dataset = original_dataframe.copy()\n",
    "        loop_dataset[\"question\"] = loop_dataset[\"question\"].apply(lambda x: apply_augmentation(x, random_augmentation=random_augmentation))\n",
    "    \n",
    "        df_augmented = pd.concat([df_augmented, loop_dataset], axis=0).reset_index(drop=True)\n",
    "\n",
    "        if checkpoints: \n",
    "            # For notebook\n",
    "            #folder_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'Datasets')) \n",
    "            #save_path = os.path.join(folder_path, 'instagram_data.csv')\n",
    "            if not save_path:\n",
    "                save_path = \"Datasets/dataset\"\n",
    "                df_augmented.to_csv(save_path, index=False)\n",
    "                continue\n",
    "            \n",
    "            # For the parallel augmentation\n",
    "            # Path would be chunks/chunk_1 , chunks/chunk_2 etc..\n",
    "            df_augmented.to_csv(save_path, index=False)\n",
    "    \n",
    "    # Sort the dataset for sequential data.\n",
    "    df_augmented = df_augmented.sort_values(by='timestamp').reset_index(drop=True)\n",
    "    df_augmented.drop_duplicates(inplace=True)\n",
    "\n",
    "    return df_augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmented_100 = augment_data(dataset, random_augmentation=True, samples=200) # Optimization is not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up efficient processing with various optimization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting processing our datasets, we will work on the optimization and speeding of our code. Since the processing functions requires a lot of computations, we will work on it.\n",
    "\n",
    "* Added cache to avoid performing computations multiple times\n",
    "* Added parallel processing\n",
    "* Added auto memory managment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "import multiprocess as mp # NOT multiprocessing to avoid __main__ improtable problem by the children \n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, chunk_size = os.cpu_count()):\n",
    "    chunks = np.array_split(df, chunk_size)\n",
    "    return chunks\n",
    "\n",
    "def process_parallel(df_chunk, **kwargs):\n",
    "    return augment_data(df_chunk, **kwargs)\n",
    "\n",
    "df_chunks = split_dataframe(dataset)\n",
    "kwargs: dict = {\n",
    "    \"augmentation_factor\": 2,\n",
    "    \"random_augmentation\": True,\n",
    "    \"samples\": None, \n",
    "    \"checkpoints\": True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_wrapper(df: pd.DataFrame, save_path: str, **kwargs):\n",
    "      return augment_data(df, save_path, **kwargs)\n",
    "\n",
    "def parallel_computing(df, func, num_partitions=mp.cpu_count()-2, **kwargs):\n",
    "    df_split = np.array_split(df, num_partitions) \n",
    "    save_paths = [f\"data_chunks/chunk_{i+1}\" for i in range(num_partitions)] # Create save_paths for each partition\n",
    "    \n",
    "    func_with_kwargs = partial(func, **kwargs)\n",
    "\n",
    "    # Create a pool of workers\n",
    "    pool = mp.Pool(num_partitions)\n",
    "    try:\n",
    "      # Apply the function to each partition in parallel\n",
    "      pool.starmap(partial(func, **kwargs), [(df_split[i], save_paths[i]) for i in range(num_partitions)])\n",
    "      pool.join()\n",
    "      \n",
    "      return df\n",
    "    # Close the pool and wait for the work to finish\n",
    "    except Exception as e:\n",
    "      print(\"parallel_computing EXCEPTION: \" + str(e))\n",
    "      pool.terminate()\n",
    "\n",
    "      pool.join()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = parallel_computing(dataset, augmentation_wrapper, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all processings\n",
    "pool = mp.Pool(mp.cpu_count()-2)\n",
    "pool.terminate()\n",
    "pool.join()\n",
    "print(\"All workers have been terminated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the speed of regular vs parallel processing\n",
    "test_samples = 400\n",
    "\n",
    "\n",
    "# Speed test\n",
    "regular_start = time.time()\n",
    "augmented_dataset = augment_data(dataset, random_augmentation=True, samples=test_samples)\n",
    "regular_finish = time.time()\n",
    "\n",
    "parallel_start = time.time()\n",
    "kwargs[\"samples\"] = test_samples\n",
    "#kwargs[\"samples\"] = test_samples / (mp.cpu_count()-2)\n",
    "augmented_dataset = parallel_computing(dataset, augmentation_wrapper, **kwargs)\n",
    "parallel_finish = time.time()\n",
    "\n",
    "print(f\"Regular: {regular_finish - regular_start:.2f} seconds\")\n",
    "print(f\"Parallel: {parallel_finish - parallel_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
