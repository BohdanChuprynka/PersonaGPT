{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is for analyzing the steps during processing data. It contains a lot more documentation and code than the original script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_filter = os.getenv('KEYS_TO_FILTER').split(',')\n",
    "concatenated_path = os.getenv('CONCATENATED_PATH')\n",
    "dataset_path = \"Datasets/concatenated.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(dataset_path)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "      return re.sub(r'http\\S+', 'redacted', text)\n",
    "# For non-english datasets\n",
    "def remove_english_words(text):\n",
    "    # Looks for all English words and removes them.\n",
    "    pattern = r'\\b[a-zA-Z]+\\b'\n",
    "    return re.sub(pattern, '', text)\n",
    "def delete_html_tags(text):\n",
    "    clean_text = re.sub(r'<.*?>', '', text)\n",
    "    return clean_text\n",
    "def remove_mention(text):\n",
    "  mention_regex = r\"@\\w+\"\n",
    "  return re.sub(mention_regex, \"/mention\", text)\n",
    "def redact_email(text): \n",
    "    return re.sub(r'\\S+@\\S+', '/email', text)\n",
    "# def remove_password(text): \n",
    "#     copy_text = text\n",
    "#     pass_pattern = r'[A-Za-z0-9@#$%^&+=]{8,}'\n",
    "#     text_ = re.sub(pass_pattern, '', text)\n",
    "#     return text_\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "def sen_len_threshold(text, char_min=16, char_limit=512): # Can be used for better tuning. \n",
    "    text = str(text)\n",
    "    # Removes sentences if between char_min and char_limit.\n",
    "    clean_text = text if char_min <= len(text) <= char_limit else None\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, ' ', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sensitive_words(sentence, replacement='CENSORED', keys_to_filter=keys_to_filter):\n",
    "    \"\"\"\n",
    "    Create a list of sensitive words 'keys_to_filter' from .env file \n",
    "    Replaces sensitive for you words with 'CENSORED'\n",
    "\n",
    "    Parameters: \n",
    "        sentence \n",
    "        replacement: str = words that will be substituted instead of the sensitive words   \n",
    "    \"\"\"\n",
    "    words = set(keys_to_filter)\n",
    "    sentence_words = sentence.split()\n",
    "    \n",
    "    modified_sentence = [\n",
    "        replacement if word in words else word for word in sentence_words\n",
    "    ]\n",
    "    \n",
    "    # Join the list back into a sentence\n",
    "    return ' '.join(modified_sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since \" \" rows don't count as NAN, we should identify them by ourselves.\n",
    "def drop_space_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "      \"\"\"Identifies and drops ' ' rows in the DataFrame\"\"\"\n",
    "      space_rows = df['Message'] == ' '\n",
    "      df_filtered = df[~pd.Series(space_rows)].reset_index(drop=True)\n",
    "\n",
    "      return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "      text = remove_english_words(text)\n",
    "      text = redact_email(text)\n",
    "      text = remove_urls(text)\n",
    "      text = remove_mention(text)\n",
    "      text = delete_html_tags(text)\n",
    "      text = filter_sensitive_words(text)\n",
    "      text = remove_whitespace(text)\n",
    "      \n",
    "      return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    import time \n",
    "    dataset_copy = df.copy()\n",
    "    start_time= time.time()\n",
    "    df['Message'] = df['Message'].apply(preprocess_data)\n",
    "    df[\"Message\"] = df[\"Message\"].apply(lambda x: remove_emojis(str(x)) if isinstance(x, str) else ' ')\n",
    "    df = drop_space_rows(df)\n",
    "    df.to_csv(concatenated_path, index=False)\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total time for processing: {total_time:.2f} seconds\")\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copy = dataset.copy() # For visual purposes\n",
    "dataset = preprocess_dataset(dataset)\n",
    "\n",
    "b_length = len(dataset_copy)\n",
    "a_length = len(dataset)\n",
    "b_mean_length = np.mean(dataset_copy['Message'].str.len())\n",
    "a_mean_length = np.mean(dataset['Message'].str.len())\n",
    "b_max_length = np.max(dataset_copy['Message'].str.len())\n",
    "a_max_length = np.max(dataset['Message'].str.len())\n",
    "longest_sentence_index = dataset['Message'].str.len().idxmax()\n",
    "longest_sentence = dataset['Message'].iloc[longest_sentence_index]\n",
    "\n",
    "\n",
    "print(f\"Changes (Before/After) processing:\")\n",
    "print(f\"Length: {b_length} -> {a_length}\")\n",
    "print(f\"Median length: {b_mean_length:.2f} -> {a_mean_length:.2f}\")\n",
    "print(f\"Max sentence length: {b_max_length} -> {a_max_length}\")\n",
    "print(f\"Nan values: {dataset_copy.isna().sum().sum()} -> {dataset.isna().sum().sum()}\")\n",
    "print(f\"Longest sentence: {len(longest_sentence)} chars: {longest_sentence}\")\n",
    "\n",
    "del dataset_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving into Question / Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating a column with time difference between messages \n",
    "To correctly assign the context.\n",
    "\"\"\"\n",
    "dataset = dataset.sort_values(by=['Date']).reset_index(drop=True)\n",
    "\n",
    "dataset['Date'] = pd.to_datetime(dataset['Date'], format='ISO8601')\n",
    "\n",
    "reference_time = dataset['Date'].min()\n",
    "dataset['time_diff_seconds'] = dataset['Date'] - reference_time\n",
    "# Converts into hours difference\n",
    "dataset['time_diff_seconds'] = dataset['time_diff_seconds'].apply(lambda x: int(x.total_seconds()))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sentences(df: pd.DataFrame) -> pd.DataFrame:\n",
    "      \"\"\"\n",
    "      Takes a pandas dataframe with a messages column and returns separated rows with question / answer columns\n",
    "      Args: \n",
    "            dataset: pd.DataFrame\n",
    "            Dataset should contain a messages column and first row with identification who sent a message.\n",
    "\n",
    "\n",
    "\n",
    "      Returns:\n",
    "            dataset: pd.DataFrame\n",
    "            \n",
    "            Dataset divided into question / answer columns.\n",
    "      \"\"\"\n",
    "\n",
    "      separated_dataset = pd.DataFrame(columns=['question', 'answer', 'timestamp', 'Sent_by_me', 'time_diff_seconds'])\n",
    "\n",
    "      # Make the first row the first question (All questions become even, answers->odds)\n",
    "      if df[\"Sent_by_me\"].iloc[0]: \n",
    "            df = df.drop(df.index[0]).reset_index(drop=True)\n",
    "\n",
    "      questions_df = df[df.index % 2 == 0].reset_index(drop=True)\n",
    "      answers_df = df[df.index % 2 == 1].reset_index(drop=True)\n",
    "\n",
    "      min_length = min(len(questions_df), len(answers_df))\n",
    "\n",
    "      separated_dataset = pd.concat(\n",
    "     [\n",
    "        questions_df[\"Message\"][:min_length].rename(\"question\"),\n",
    "        answers_df[\"Message\"][:min_length].rename(\"answer\"),\n",
    "        df[\"Date\"][:min_length].rename(\"timestamp\"),\n",
    "        df[\"Sent_by_me\"][:min_length].rename(\"Sent_by_me\"),\n",
    "        df[\"time_diff_seconds\"][:min_length].rename(\"time_diff_seconds\")\n",
    "     ], axis=1\n",
    ")\n",
    "\n",
    "      return separated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_dataset = separate_sentences(dataset)\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding column with previous context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_context(df: pd.DataFrame, context_size: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a column with previous context to the DataFrame.\n",
    "    \n",
    "    The context is based on the previous messages. If the time difference \n",
    "    between messages is more than 2 hours, it's considered the start of a \n",
    "    new conversation, and the first row of that new conversation will have \n",
    "    no context. Subsequent messages in the conversation will have context.\n",
    "    \"\"\"\n",
    "    \n",
    "    context_list = []\n",
    "    last_time = None  # Track the last message time to determine time gaps\n",
    "    \n",
    "    for index in range(len(df)):\n",
    "        if index == 0:\n",
    "            # No context for the very first message\n",
    "            context_list.append(None)\n",
    "            last_time = df.loc[index, \"time_diff_seconds\"]\n",
    "            continue\n",
    "        \n",
    "        # Calculate the time difference from the previous row\n",
    "        time_diff = df.loc[index, \"time_diff_seconds\"] - last_time\n",
    "        last_time = df.loc[index, \"time_diff_seconds\"]\n",
    "\n",
    "        # If time_diff is more than 6 hours, consider it a new conversation\n",
    "        if time_diff > 21600:\n",
    "            context_list.append(None)  # Start of a new conversation, no context\n",
    "        else:\n",
    "            # Create context from the previous messages within the context size\n",
    "            start_index = max(index - context_size, 0)\n",
    "            context = df.loc[start_index:index - 1, [\"question\", \"answer\"]]\n",
    "\n",
    "            # Build the context string from previous rows\n",
    "            message = []\n",
    "            for key, (question, answer) in enumerate(zip(context[\"question\"], context[\"answer\"])):\n",
    "                message.append(f\"Q{key + 1}: {question}. A{key + 1}: {answer} || \")\n",
    "\n",
    "            # Append the concatenated message as the context\n",
    "            context_list.append(\" \".join(message))\n",
    "\n",
    "    # Handle 1st row None (diff seconds in 0 index is 0, then 1 is None).\n",
    "    context = df.loc[0, [\"question\", \"answer\"]]\n",
    "    question, answer = context[\"question\"], context[\"answer\"]\n",
    "    context_list[1] = (f\"Q{1}: {question}. A{1}: {answer} || \")\n",
    "    \n",
    "    # Add the context as a new column\n",
    "    df[\"context\"] = context_list\n",
    "\n",
    "    # Replace any empty or missing contexts with \"Missing Context\" if desired\n",
    "    df[\"context\"] = df[\"context\"].apply(lambda x: \"Time Gap\" if pd.isna(x) else x)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_dataset = add_context(separated_dataset)\n",
    "separated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time_gaps = (separated_dataset[\"context\"] == \"Time Gap\").sum()\n",
    "total_time_gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation \n",
    "and continue of processing\n",
    "\n",
    "Inspired by: \n",
    "https://github.com/jasonwei20/eda_nlp\n",
    "\n",
    "Methods: \n",
    "1. Back-translation\n",
    "2. Synonym replacement\n",
    "3. Word Swap\n",
    "4. Sentence shuffle\n",
    "\n",
    "Remember that this notebook is designed to work with ukrainian language dataset, and not all techniques will work for English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = separated_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def remove_double_commas(text: str) -> str:\n",
    "    \"\"\"Removes double commas from the text.\"\"\"\n",
    "    return text.replace(\",,\", \",\")\n",
    "\n",
    "def split_sentences(text: str) -> list:\n",
    "    \"\"\"Splits the text into sentences by commas, handling empty strings gracefully.\"\"\"\n",
    "    return [sentence.strip() for sentence in text.split(',') if sentence.strip()]\n",
    "\n",
    "def shuffle_sentence(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes double commas, splits the text into sentences, shuffles them,\n",
    "    and joins them back into a shuffled sentence.\n",
    "    \"\"\"\n",
    "    # Step 1: Clean and split the sentences\n",
    "    clean_text = remove_double_commas(text)\n",
    "    sentences = split_sentences(clean_text)\n",
    "\n",
    "    # Step 2: Shuffle the sentences\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "    # Step 3: Join shuffled sentences back into a single string\n",
    "    return \", \".join(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> for shuffle example.\n",
    "\n",
    "<!--\n",
    "text = \"This is a test,, sentence, another part,, and more text. Це просто тест, такий вот тест\"\n",
    "shuffled_text = shuffle_sentence(text)\n",
    "\n",
    "print(f\"Before: {text}\")\n",
    "print(f\"After: {shuffled_text}\")\n",
    "\n",
    "Outputs: \n",
    "Before: This is a test,, sentence, another part,, and more text. Це просто тест, такий вот тест\n",
    "After: and more text. Це просто тест, another part, This is a test, sentence, такий вот тест\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to download data for a language\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def download(model_name):\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "# download model for English -> Ukrainian\n",
    "first_tokenizer, first_model = download('Helsinki-NLP/opus-mt-uk-en')\n",
    "# download model for Ukrainian -> English\n",
    "second_tokenizer, second_model = download('Helsinki-NLP/opus-mt-en-uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_batch_texts(language_code, batch_texts):\n",
    "    formated_batch = [f\">>{language_code}<< {batch_texts}\" for sentence in batch_texts]\n",
    "\n",
    "    return formated_batch\n",
    "\n",
    "def translate(batch_texts, model, tokenizer, language):\n",
    "    \"\"\"Translate texts into a target language\"\"\"\n",
    "    # Format the text as expected by the model\n",
    "    batched_text = format_batch_texts(language, batch_texts)\n",
    "\n",
    "    # Translate\n",
    "    translated = [model.generate(**tokenizer(batch_texts, return_tensors=\"pt\", padding=True)) for sentence in batched_text]\n",
    "\n",
    "    # Decode (tokens to text)\n",
    "    translated_texts = tokenizer.batch_decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_texts\n",
    "\n",
    "def back_translate(texts, from_language=\"uk\", to_language = \"en\"):\n",
    "    \"\"\"Implements back translation\"\"\"\n",
    "    # Translate from source to target language\n",
    "    if from_language == \"en\":\n",
    "        translated = translate(texts, second_model, second_tokenizer, from_language)\n",
    "        back_translated = translate(translated, first_model, first_tokenizer, to_language)\n",
    "    else: \n",
    "        translated = translate(texts, first_model, first_tokenizer, from_language)\n",
    "        back_translated = translate(translated, second_model, second_tokenizer, to_language)\n",
    "\n",
    "\n",
    "    return back_translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> for back-translation example.\n",
    "\n",
    "<!--\n",
    "# Perform back-translation (Ukrainian to English to Ukrainian)\n",
    "texts = [\"Це перше речення яке ти маєш перекласти.\",\n",
    "         \"Воно є дуже просте та правильно сформульованею.\"]\n",
    "back_translated_texts = back_translate(texts)\n",
    "texts = [\"This is the first sentence you should translate\", \n",
    "        \"It is simple and correctly formulated\"]\n",
    "back_translated_texts_en = back_translate(texts, \"en\", \"uk\")\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Text:\", texts)\n",
    "print(\"Back-Translated Text:\", back_translated_texts)\n",
    "print(\"-----------------\")\n",
    "print(\"Original Text:\", texts)\n",
    "print(\"Back-Translated Text:\", back_translated_texts_en)\n",
    "\n",
    "Outputs:\n",
    "Original Text: ['This is the first sentence you should translate', 'It is simple and correctly formulated']\n",
    "Back-Translated Text: ['Це перше речення, яке ви маєте перекласти.', 'Вона дуже проста і добре сформульована.']\n",
    "-----------------\n",
    "Original Text: ['This is the first sentence you should translate', 'It is simple and correctly formulated']\n",
    "Back-Translated Text: ['This is the first sentence you have to translate.', \"It's simple and correctly formulated.\"]\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonyms using word embeddings (word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating augmentation rows and concatenating them with dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(df, augmentation_factor=3):\n",
    "    # Add new columns with augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_word(sentence):\n",
    "    \"\"\"Pops a random word from the sentence\"\"\"\n",
    "    words = sentence.split()\n",
    "    total_words = len(words)\n",
    "\n",
    "    random_probs = np.random.rand(total_words)\n",
    "    p = random_probs / np.sum(random_probs)  # Normalize to create valid probabilities\n",
    "\n",
    "    chosen_word_index = np.random.choice(len(words), size=1, replace=False, p=p)[0]\n",
    "\n",
    "    chosen_word = words.pop(chosen_word_index)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Example: \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
