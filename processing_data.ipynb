{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is for analyzing the steps during processing data. It contains a lot more documentation and code than the original script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"Datasets/stopwords_ua_set.txt\"):\n",
    "      !wget -P\"Datasets/\" https://raw.githubusercontent.com/skupriienko/Ukrainian-Stopwords/refs/heads/master/stopwords_ua_set.txt\n",
    "\n",
    "with open('Datasets/stopwords_ua_set.txt', 'r') as file:\n",
    "    ukrainian_stop_words = file.read().splitlines()[0]\n",
    "\n",
    "keys_to_filter = os.getenv('KEYS_TO_FILTER').split(',')\n",
    "concatenated_path = os.getenv('CONCATENATED_PATH')\n",
    "dataset_path = \"Datasets/concatenated.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(dataset_path)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "      return re.sub(r'http\\S+', 'redacted', text)\n",
    "# For non-english datasets\n",
    "def remove_english_words(text):\n",
    "    # Looks for all English words and removes them.\n",
    "    pattern = r'\\b[a-zA-Z]+\\b'\n",
    "    return re.sub(pattern, '', text)\n",
    "def delete_html_tags(text):\n",
    "    clean_text = re.sub(r'<.*?>', '', text)\n",
    "    return clean_text\n",
    "def remove_mention(text):\n",
    "  mention_regex = r\"@\\w+\"\n",
    "  return re.sub(mention_regex, \"/mention\", text)\n",
    "def redact_email(text): \n",
    "    return re.sub(r'\\S+@\\S+', '/email', text)\n",
    "# def remove_password(text): \n",
    "#     copy_text = text\n",
    "#     pass_pattern = r'[A-Za-z0-9@#$%^&+=]{8,}'\n",
    "#     text_ = re.sub(pass_pattern, '', text)\n",
    "#     return text_\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "def sen_len_threshold(text, char_min=16, char_limit=512): # Can be used for better tuning. \n",
    "    text = str(text)\n",
    "    # Removes sentences if between char_min and char_limit.\n",
    "    clean_text = text if char_min <= len(text) <= char_limit else None\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, ' ', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sensitive_words(sentence, replacement='CENSORED', keys_to_filter=keys_to_filter):\n",
    "    \"\"\"\n",
    "    Create a list of sensitive words 'keys_to_filter' from .env file \n",
    "    Replaces sensitive for you words with 'CENSORED'\n",
    "\n",
    "    Parameters: \n",
    "        sentence \n",
    "        replacement: str = words that will be substituted instead of the sensitive words   \n",
    "    \"\"\"\n",
    "    words = set(keys_to_filter)\n",
    "    sentence_words = sentence.split()\n",
    "    \n",
    "    modified_sentence = [\n",
    "        replacement if word in words else word for word in sentence_words\n",
    "    ]\n",
    "    \n",
    "    # Join the list back into a sentence\n",
    "    return ' '.join(modified_sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since \" \" rows don't count as NAN, we should identify them by ourselves.\n",
    "def drop_space_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "      \"\"\"Identifies and drops ' ' rows in the DataFrame\"\"\"\n",
    "      space_rows = df['Message'] == ' '\n",
    "      df_filtered = df[~pd.Series(space_rows)].reset_index(drop=True)\n",
    "\n",
    "      return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "      text = remove_english_words(text)\n",
    "      text = redact_email(text)\n",
    "      text = remove_urls(text)\n",
    "      text = remove_mention(text)\n",
    "      text = delete_html_tags(text)\n",
    "      text = filter_sensitive_words(text)\n",
    "      text = remove_whitespace(text)\n",
    "      \n",
    "      return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    import time \n",
    "    dataset_copy = df.copy()\n",
    "    start_time= time.time()\n",
    "    df['Message'] = df['Message'].apply(preprocess_data)\n",
    "    df[\"Message\"] = df[\"Message\"].apply(lambda x: remove_emojis(str(x)) if isinstance(x, str) else ' ')\n",
    "    df = drop_space_rows(df)\n",
    "    df.to_csv(concatenated_path, index=False)\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total time for processing: {total_time:.2f} seconds\")\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copy = dataset.copy() # For visual purposes\n",
    "dataset = preprocess_dataset(dataset)\n",
    "\n",
    "b_length = len(dataset_copy)\n",
    "a_length = len(dataset)\n",
    "b_mean_length = np.mean(dataset_copy['Message'].str.len())\n",
    "a_mean_length = np.mean(dataset['Message'].str.len())\n",
    "b_max_length = np.max(dataset_copy['Message'].str.len())\n",
    "a_max_length = np.max(dataset['Message'].str.len())\n",
    "longest_sentence_index = dataset['Message'].str.len().idxmax()\n",
    "longest_sentence = dataset['Message'].iloc[longest_sentence_index]\n",
    "\n",
    "\n",
    "print(f\"Changes (Before/After) processing:\")\n",
    "print(f\"Length: {b_length} -> {a_length}\")\n",
    "print(f\"Median length: {b_mean_length:.2f} -> {a_mean_length:.2f}\")\n",
    "print(f\"Max sentence length: {b_max_length} -> {a_max_length}\")\n",
    "print(f\"Nan values: {dataset_copy.isna().sum().sum()} -> {dataset.isna().sum().sum()}\")\n",
    "print(f\"Longest sentence: {len(longest_sentence)} chars: {longest_sentence}\")\n",
    "\n",
    "del dataset_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving into Question / Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating a column with time difference between messages \n",
    "To correctly assign the context.\n",
    "\"\"\"\n",
    "dataset = dataset.sort_values(by=['Date']).reset_index(drop=True)\n",
    "\n",
    "dataset['Date'] = pd.to_datetime(dataset['Date'], format='ISO8601')\n",
    "\n",
    "reference_time = dataset['Date'].min()\n",
    "dataset['time_diff_seconds'] = dataset['Date'] - reference_time\n",
    "# Converts into hours difference\n",
    "dataset['time_diff_seconds'] = dataset['time_diff_seconds'].apply(lambda x: int(x.total_seconds()))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sentences(df: pd.DataFrame) -> pd.DataFrame:\n",
    "      \"\"\"\n",
    "      Takes a pandas dataframe with a messages column and returns separated rows with question / answer columns\n",
    "      Args: \n",
    "            dataset: pd.DataFrame\n",
    "            Dataset should contain a messages column and first row with identification who sent a message.\n",
    "\n",
    "\n",
    "\n",
    "      Returns:\n",
    "            dataset: pd.DataFrame\n",
    "            \n",
    "            Dataset divided into question / answer columns.\n",
    "      \"\"\"\n",
    "\n",
    "      separated_dataset = pd.DataFrame(columns=['question', 'answer', 'timestamp', 'Sent_by_me', 'time_diff_seconds'])\n",
    "\n",
    "      # Make the first row the first question (All questions become even, answers->odds)\n",
    "      if df[\"Sent_by_me\"].iloc[0]: \n",
    "            df = df.drop(df.index[0]).reset_index(drop=True)\n",
    "\n",
    "      questions_df = df[df.index % 2 == 0].reset_index(drop=True)\n",
    "      answers_df = df[df.index % 2 == 1].reset_index(drop=True)\n",
    "\n",
    "      min_length = min(len(questions_df), len(answers_df))\n",
    "\n",
    "      separated_dataset = pd.concat(\n",
    "     [\n",
    "        questions_df[\"Message\"][:min_length].rename(\"question\"),\n",
    "        answers_df[\"Message\"][:min_length].rename(\"answer\"),\n",
    "        df[\"Date\"][:min_length].rename(\"timestamp\"),\n",
    "        df[\"Sent_by_me\"][:min_length].rename(\"Sent_by_me\"),\n",
    "        df[\"time_diff_seconds\"][:min_length].rename(\"time_diff_seconds\")\n",
    "     ], axis=1\n",
    ")\n",
    "\n",
    "      return separated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = separate_sentences(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding column with previous context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_context(df: pd.DataFrame, context_size: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a column with previous context to the DataFrame.\n",
    "    \n",
    "    The context is based on the previous messages. If the time difference \n",
    "    between messages is more than 2 hours, it's considered the start of a \n",
    "    new conversation, and the first row of that new conversation will have \n",
    "    no context. Subsequent messages in the conversation will have context.\n",
    "    \"\"\"\n",
    "    \n",
    "    context_list = []\n",
    "    last_time = None  # Track the last message time to determine time gaps\n",
    "    \n",
    "    for index in range(len(df)):\n",
    "        if index == 0:\n",
    "            # No context for the very first message\n",
    "            context_list.append(None)\n",
    "            last_time = df.loc[index, \"time_diff_seconds\"]\n",
    "            continue\n",
    "        \n",
    "        # Calculate the time difference from the previous row\n",
    "        time_diff = df.loc[index, \"time_diff_seconds\"] - last_time\n",
    "        last_time = df.loc[index, \"time_diff_seconds\"]\n",
    "\n",
    "        # If time_diff is more than 6 hours, consider it a new conversation\n",
    "        if time_diff > 21600:\n",
    "            context_list.append(None)  # Start of a new conversation, no context\n",
    "        else:\n",
    "            # Create context from the previous messages within the context size\n",
    "            start_index = max(index - context_size, 0)\n",
    "            context = df.loc[start_index:index - 1, [\"question\", \"answer\"]]\n",
    "\n",
    "            # Build the context string from previous rows\n",
    "            message = []\n",
    "            for key, (question, answer) in enumerate(zip(context[\"question\"], context[\"answer\"])):\n",
    "                message.append(f\"Q{key + 1}: {question}. A{key + 1}: {answer} || \")\n",
    "\n",
    "            # Append the concatenated message as the context\n",
    "            context_list.append(\" \".join(message))\n",
    "\n",
    "    # Handle 1st row None (diff seconds in 0 index is 0, then 1 is None).\n",
    "    context = df.loc[0, [\"question\", \"answer\"]]\n",
    "    question, answer = context[\"question\"], context[\"answer\"]\n",
    "    context_list[1] = (f\"Q{1}: {question}. A{1}: {answer} || \")\n",
    "    \n",
    "    # Add the context as a new column\n",
    "    df[\"context\"] = context_list\n",
    "\n",
    "    # Replace any empty or missing contexts with \"Missing Context\" if desired\n",
    "    df[\"context\"] = df[\"context\"].apply(lambda x: \"Time Gap\" if pd.isna(x) else x)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = add_context(dataset)\n",
    "dataset.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time_gaps = (dataset[\"context\"] == \"Time Gap\").sum()\n",
    "total_time_gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation \n",
    "and continue of processing\n",
    "\n",
    "Resources: \n",
    "https://arxiv.org/pdf/1901.11196\n",
    "\n",
    "Methods: \n",
    "1. Back-translation\n",
    "2. Synonym replacement\n",
    "3. Word Swap\n",
    "4. Sentence shuffle\n",
    "\n",
    "Remember that this notebook is designed to work with ukrainian language dataset, and not all techniques will work for English language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def remove_double_commas(text: str) -> str:\n",
    "    \"\"\"Removes double commas from the text.\"\"\"\n",
    "    return text.replace(\",,\", \",\")\n",
    "\n",
    "def split_sentences(text: str) -> list:\n",
    "    \"\"\"Splits the text into sentences by commas, handling empty strings gracefully.\"\"\"\n",
    "    return [sentence.strip() for sentence in text.split(',') if sentence.strip()]\n",
    "\n",
    "def shuffle_sentence(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes double commas, splits the text into sentences, shuffles them,\n",
    "    and joins them back into a shuffled sentence.\n",
    "    \"\"\"\n",
    "    # Step 1: Clean and split the sentences\n",
    "    clean_text = remove_double_commas(text)\n",
    "    sentences = split_sentences(clean_text)\n",
    "\n",
    "    # Step 2: Shuffle the sentences\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "    # Step 3: Join shuffled sentences back into a single string\n",
    "    return \", \".join(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> for shuffle example.\n",
    "\n",
    "<!--\n",
    "text = \"This is a test,, sentence, another part,, and more text. Це просто тест, такий вот тест\"\n",
    "shuffled_text = shuffle_sentence(text)\n",
    "\n",
    "print(f\"Before: {text}\")\n",
    "print(f\"After: {shuffled_text}\")\n",
    "\n",
    "Outputs: \n",
    "Before: This is a test,, sentence, another part,, and more text. Це просто тест, такий вот тест\n",
    "After: and more text. Це просто тест, another part, This is a test, sentence, такий вот тест\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-translation using MarianMTModel\n",
    "**Not unilizing in the project because of the slow generation time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> to see MarianMTModel\n",
    "\n",
    "\n",
    "<!--\n",
    "# Helper function to download data for a language\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def download(model_name):\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "# download model for English -> Ukrainian\n",
    "first_tokenizer, first_model = download('Helsinki-NLP/opus-mt-uk-en')\n",
    "# download model for Ukrainian -> English\n",
    "second_tokenizer, second_model = download('Helsinki-NLP/opus-mt-en-uk')\n",
    "\n",
    "def format_batch_texts(language_code, batch_text):\n",
    "    formated_batch = [f\">>{language_code}<< {batch_text}\"]\n",
    "\n",
    "    return formated_batch\n",
    "\n",
    "def translate(batch_texts, model, tokenizer, language):\n",
    "    \"\"\"Translate texts into a target language\"\"\"\n",
    "    # Format the text as expected by the model\n",
    "    batched_text = format_batch_texts(language, batch_texts)\n",
    "\n",
    "    # Translate\n",
    "    translated = [model.generate(**tokenizer(batch_texts, return_tensors=\"pt\", padding=True)) for sentence in batched_text]\n",
    "\n",
    "    # Decode (tokens to text)\n",
    "    translated_texts = tokenizer.batch_decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_texts\n",
    "\n",
    "def back_translate(texts, from_language=\"uk\", to_language = \"en\"):\n",
    "    \"\"\"Implements back translation\"\"\"\n",
    "    # Translate from source to target language\n",
    "    if from_language == \"uk\": \n",
    "        translated = translate(texts, first_model, first_tokenizer, from_language)\n",
    "        back_translated = translate(translated, second_model, second_tokenizer, to_language)[0]\n",
    "        return back_translated\n",
    "    \n",
    "    translated = translate(texts, second_model, second_tokenizer, from_language)\n",
    "    back_translated = translate(translated, first_model, first_tokenizer, to_language)[0]\n",
    "\n",
    "    return back_translated\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> for back-translation example with MarianMTModel\n",
    "\n",
    "<!--\n",
    "# Perform back-translation (Ukrainian to English to Ukrainian)\n",
    "texts = [\"Це перше речення яке ти маєш перекласти.\",\n",
    "         \"Воно є дуже просте та правильно сформульованею.\"]\n",
    "back_translated_texts = back_translate(texts)\n",
    "texts = [\"This is the first sentence you should translate\", \n",
    "        \"It is simple and correctly formulated\"]\n",
    "back_translated_texts_en = back_translate(texts, \"en\", \"uk\")\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Text:\", texts)\n",
    "print(\"Back-Translated Text:\", back_translated_texts)\n",
    "print(\"-----------------\")\n",
    "print(\"Original Text:\", texts)\n",
    "print(\"Back-Translated Text:\", back_translated_texts_en)\n",
    "\n",
    "Outputs:\n",
    "Original Text: ['This is the first sentence you should translate', 'It is simple and correctly formulated']\n",
    "Back-Translated Text: ['Це перше речення, яке ви маєте перекласти.', 'Вона дуже проста і добре сформульована.']\n",
    "-----------------\n",
    "Original Text: ['This is the first sentence you should translate', 'It is simple and correctly formulated']\n",
    "Back-Translated Text: ['This is the first sentence you have to translate.', \"It's simple and correctly formulated.\"]\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swap and Pop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_word(sentence): \n",
    "    \"\"\"Swaps two random words in the sentence\"\"\"\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return sentence\n",
    "\n",
    "    idx1, idx2 = np.random.choice(len(words), size=2, replace=False)\n",
    "    words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(sentence, stop_words=ukrainian_stop_words) -> str:\n",
    "    \"\"\" Returns two lists: words with stopwords and words without stopwords\"\"\"\n",
    "    words = sentence.split()\n",
    "    filtered_stopwords = [word for word in sentence.split() if word.lower() not in stop_words]\n",
    "    return words, filtered_stopwords\n",
    "\n",
    "def pop_word(sentence, word_swap: bool = False):\n",
    "    \"\"\"Pops a random word from the sentence\"\"\"\n",
    "\n",
    "    words, stop_words = filter_stopwords(sentence)\n",
    "\n",
    "    if stop_words: \n",
    "        remove_index = np.random.choice(stop_words, size=1, replace=False)[0]\n",
    "        words.remove(remove_index)\n",
    "    else: \n",
    "        return sentence\n",
    "\n",
    "\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> for word elimination example.\n",
    "\n",
    "<!--\n",
    "# Example: \n",
    "\n",
    "example = \"Це є експериментальним реченням. Воно прикольне))\" # Stopwords work only with ukrainian language.\n",
    "example = pop_word(example)\n",
    "\n",
    "print(example)\n",
    "\n",
    "Outputs: \n",
    "\"is a random sentence\"\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> for word swap example.\n",
    "\n",
    "<!--\n",
    "# Example: \n",
    "\n",
    "example = \"This is a random sentence\"\n",
    "example = swap_word(example)\n",
    "\n",
    "print(example)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back translation and Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_CODES = {\n",
    "    'afrikaans': 'af',\n",
    "    'albanian': 'sq',\n",
    "    'amharic': 'am',\n",
    "    'arabic': 'ar',\n",
    "    'armenian': 'hy',\n",
    "    'azerbaijani': 'az',\n",
    "    'basque': 'eu',\n",
    "    'belarusian': 'be',\n",
    "    'bengali': 'bn',\n",
    "    'bosnian': 'bs',\n",
    "    'bulgarian': 'bg',\n",
    "    'catalan': 'ca',\n",
    "    'cebuano': 'ceb',\n",
    "    'chichewa': 'ny',\n",
    "    'chinese (simplified)': 'zh-cn',\n",
    "    'chinese (traditional)': 'zh-tw',\n",
    "    'corsican': 'co',\n",
    "    'croatian': 'hr',\n",
    "    'czech': 'cs',\n",
    "    'danish': 'da',\n",
    "    'dutch': 'nl',\n",
    "    'english': 'en',\n",
    "    'esperanto': 'eo',\n",
    "    'estonian': 'et',\n",
    "    'filipino': 'tl',\n",
    "    'finnish': 'fi',\n",
    "    'french': 'fr',\n",
    "    'frisian': 'fy',\n",
    "    'galician': 'gl',\n",
    "    'georgian': 'ka',\n",
    "    'german': 'de',\n",
    "    'greek': 'el',\n",
    "    'gujarati': 'gu',\n",
    "    'haitian creole': 'ht',\n",
    "    'hausa': 'ha',\n",
    "    'hawaiian': 'haw',\n",
    "    'hebrew': 'he',\n",
    "    'hindi': 'hi',\n",
    "    'hmong': 'hmn',\n",
    "    'hungarian': 'hu',\n",
    "    'icelandic': 'is',\n",
    "    'igbo': 'ig',\n",
    "    'indonesian': 'id',\n",
    "    'irish': 'ga',\n",
    "    'italian': 'it',\n",
    "    'japanese': 'ja',\n",
    "    'javanese': 'jw',\n",
    "    'kannada': 'kn',\n",
    "    'kazakh': 'kk',\n",
    "    'khmer': 'km',\n",
    "    'korean': 'ko',\n",
    "    'kurdish (kurmanji)': 'ku',\n",
    "    'kyrgyz': 'ky',\n",
    "    'lao': 'lo',\n",
    "    'latin': 'la',\n",
    "    'latvian': 'lv',\n",
    "    'lithuanian': 'lt',\n",
    "    'luxembourgish': 'lb',\n",
    "    'macedonian': 'mk',\n",
    "    'malagasy': 'mg',\n",
    "    'malay': 'ms',\n",
    "    'malayalam': 'ml',\n",
    "    'maltese': 'mt',\n",
    "    'maori': 'mi',\n",
    "    'marathi': 'mr',\n",
    "    'mongolian': 'mn',\n",
    "    'myanmar (burmese)': 'my',\n",
    "    'nepali': 'ne',\n",
    "    'norwegian': 'no',\n",
    "    'odia': 'or',\n",
    "    'pashto': 'ps',\n",
    "    'persian': 'fa',\n",
    "    'polish': 'pl',\n",
    "    'portuguese': 'pt',\n",
    "    'punjabi': 'pa',\n",
    "    'romanian': 'ro',\n",
    "    'russian': 'ru',\n",
    "    'samoan': 'sm',\n",
    "    'scots gaelic': 'gd',\n",
    "    'serbian': 'sr',\n",
    "    'sesotho': 'st',\n",
    "    'shona': 'sn',\n",
    "    'sindhi': 'sd',\n",
    "    'sinhala': 'si',\n",
    "    'slovak': 'sk',\n",
    "    'slovenian': 'sl',\n",
    "    'somali': 'so',\n",
    "    'spanish': 'es',\n",
    "    'sundanese': 'su',\n",
    "    'swahili': 'sw',\n",
    "    'swedish': 'sv',\n",
    "    'tajik': 'tg',\n",
    "    'tamil': 'ta',\n",
    "    'telugu': 'te',\n",
    "    'thai': 'th',\n",
    "    'turkish': 'tr',\n",
    "    'ukrainian': 'uk',\n",
    "    'urdu': 'ur',\n",
    "    'uyghur': 'ug',\n",
    "    'uzbek': 'uz',\n",
    "    'vietnamese': 'vi',\n",
    "    'welsh': 'cy',\n",
    "    'xhosa': 'xh',\n",
    "    'yiddish': 'yi',\n",
    "    'yoruba': 'yo',\n",
    "    'zulu': 'zu'}\n",
    "\n",
    "LANGUAGES = {value:key for key, value in LANG_CODES.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "class google_translate:\n",
    "    \"\"\"\n",
    "    Performs Google Translate on a given text.\n",
    "\n",
    "    Args:\n",
    "        translate_from (str): The natural language of the text. Defaults to \"uk\". Contains auto language detection.\n",
    "        translate_to (str): The language to translate to and back from. Defaults to \"en\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, translate_from: str = \"uk\", translate_to: str = \"en\", replace_synonyms: bool = False):\n",
    "        self.native_language = translate_from\n",
    "        self.tunnel_language = translate_to \n",
    "        self.translator = Translator()\n",
    "\n",
    "        if replace_synonyms:\n",
    "            self.word2vec_model = self.install_word2vec()\n",
    "\n",
    "    \"\"\" Back-translation \"\"\"\n",
    "    # Check whether the language input is correct\n",
    "    def check_language(self, text):\n",
    "\n",
    "        if self.native_language not in LANGUAGES:  \n",
    "            self.native_language = self.translator.detect(text).lang\n",
    "            print(f\"Incorrect language. Translating from '{self.native_language}'\")\n",
    "\n",
    "            # If the back-translation is going on English text, the text will be translated from English to Spanish and back to English.\n",
    "            if self.native_language == \"en\": \n",
    "                self.tunnel_language == \"es\"\n",
    "\n",
    "                \n",
    "    def back_translate(self, text, replace_synonym: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Performs back-translation on a given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to back-translate.\n",
    "            temp_lang (str): The intermediate language for translation. Defaults to French (\"fr\").\n",
    "\n",
    "        Returns:\n",
    "            str: The back-translated text.\n",
    "        \"\"\"\n",
    "        translator = self.translator\n",
    "\n",
    "        self.check_language(text=text)\n",
    "\n",
    "        translated = self.translator.translate(text, src=self.native_language, dest=self.tunnel_language).text\n",
    "        \n",
    "        if replace_synonym: \n",
    "            #translated = self.synonym_replacement(sentence=translated) # TODO: Add English stopwords.\n",
    "            pass\n",
    "\n",
    "        back_translated = translator.translate(translated, src=self.tunnel_language, dest=self.native_language).text\n",
    "\n",
    "        return back_translated\n",
    "    \n",
    "    \"\"\" Synonym extension (Word2Vec) \"\"\"\n",
    "\n",
    "    def install_word2vec(self):\n",
    "      model_name = \"word2vec-google-news-300\"\n",
    "      print(f\"Configuring {model_name}\")\n",
    "      word2vec_model = api.load(model_name)\n",
    "\n",
    "      return word2vec_model\n",
    "\n",
    "    def synonym_replacement(self, sentence):\n",
    "        # Remove stopwords \n",
    "        words, filtered_sentence = filter_stopwords(sentence)\n",
    "        random_word_index = np.random.choice(len(filtered_sentence), size=1)[0]\n",
    "        synonym = self.word2vec_model.most_similar(filtered_sentence[random_word_index], topn=1)[0][0] # Top 5 most similar words\n",
    "        words[random_word_index] = synonym    \n",
    "\n",
    "        return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click <b>here</b> to see back-translation example.\n",
    "\n",
    "\n",
    "<!--\n",
    "translator = google_translate(translate_from=\"uk\", translate_to=\"en\")\n",
    "back_translated = translator.back_translate(\"Привіт, як воно?\") # Hello, how is it going?\n",
    "back_translated\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating augmentation rows and concatenating them with dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = google_translate(translate_from=\"uk\", translate_to=\"en\", replace_synonyms=True)\n",
    "def apply_augmentation(sentence) -> pd.DataFrame:\n",
    "    sentence = translator.back_translate(sentence)\n",
    "    sentence = shuffle_sentence(sentence)\n",
    "    sentence = swap_word(sentence)\n",
    "    sentence = pop_word(sentence)\n",
    "    # sentence = synonym_replacement(sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def speed_test(df, size=100):\n",
    "    start_time = time.time()\n",
    "    df[\"question\"] = df[\"question\"].loc[:size].apply(lambda x: apply_augmentation(x))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return \n",
    "\n",
    "def augment_data(df: pd.DataFrame, augmentation_factor: int = 5) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Augments the data by adding augmented questions.\n",
    "    \n",
    "    Parameters:\n",
    "        df: pd.DataFrame with \"question\" column\n",
    "        augmentation_factor: int = 5; how many times to augment each question.\n",
    "    \"\"\"\n",
    "\n",
    "    df_augmented = df.copy()\n",
    "    for _ in range(augmentation_factor):\n",
    "        augmented_dataset = df.copy()\n",
    "        augmented_dataset[\"question\"] = augmented_dataset[\"question\"].apply(lambda x: apply_augmentation(x))\n",
    "    \n",
    "        df_augmented = pd.concat([df_augmented, augmented_dataset], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # Sort the dataset for sequential data.\n",
    "    df_augmented = df_augmented.sort_values(by='timestamp').reset_index(drop=True)\n",
    "    df_augmented.drop_duplicates(inplace=True)\n",
    "\n",
    "    return df_augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = augment_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
