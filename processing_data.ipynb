{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is for analyzing the steps during processing data. It contains a lot more documentation and code than the original script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_filter = os.getenv('KEYS_TO_FILTER').split(',')\n",
    "concatenated_path = os.getenv('CONCATENATED_PATH')\n",
    "dataset_path = \"Datasets/concatenated.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(dataset_path)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "      return re.sub(r'http\\S+', 'redacted', text)\n",
    "def remove_english_words(text):\n",
    "    # Looks for all English words and removes them.\n",
    "    pattern = r'\\b[a-zA-Z]+\\b'\n",
    "    return re.sub(pattern, '', text)\n",
    "def delete_html_tags(text):\n",
    "    clean_text = re.sub(r'<.*?>', '', text)\n",
    "    return clean_text\n",
    "def remove_mention(text):\n",
    "  mention_regex = r\"@\\w+\"\n",
    "  return re.sub(mention_regex, \"/mention\", text)\n",
    "def redact_email(text): \n",
    "    return re.sub(r'\\S+@\\S+', '/email', text)\n",
    "# def remove_password(text): \n",
    "#     copy_text = text\n",
    "#     pass_pattern = r'[A-Za-z0-9@#$%^&+=]{8,}'\n",
    "#     text_ = re.sub(pass_pattern, '', text)\n",
    "#     return text_\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "def sen_len_threshold(text, char_min=16, char_limit=512): # Can be used for better tuning. \n",
    "    text = str(text)\n",
    "    # Removes sentences if between char_min and char_limit.\n",
    "    clean_text = text if char_min <= len(text) <= char_limit else None\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, ' ', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sensitive_words(sentence, replacement='CENSORED'):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        replacement: str = words that will be substituted instead of the sensitive words   \n",
    "    \"\"\"\n",
    "    words = set(keys_to_filter)\n",
    "    sentence_words = sentence.split()\n",
    "    \n",
    "    modified_sentence = [\n",
    "        replacement if word in words else word for word in sentence_words\n",
    "    ]\n",
    "    \n",
    "    # Join the list back into a sentence\n",
    "    return ' '.join(modified_sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_space_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "      space_rows = df['Message'] == ' '\n",
    "      df_filtered = df[~pd.Series(space_rows)].reset_index(drop=True)\n",
    "\n",
    "      return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "      text = remove_english_words(text)\n",
    "      text = redact_email(text)\n",
    "      text = remove_urls(text)\n",
    "      text = remove_mention(text)\n",
    "      text = delete_html_tags(text)\n",
    "      text = filter_sensitive_words(text)\n",
    "      text = remove_whitespace(text)\n",
    "      \n",
    "      return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    import time \n",
    "    dataset_copy = df.copy()\n",
    "    start_time= time.time()\n",
    "    df['Message'] = df['Message'].apply(preprocess_data)\n",
    "    df[\"Message\"] = df[\"Message\"].apply(lambda x: remove_emojis(str(x)) if isinstance(x, str) else ' ')\n",
    "    df = drop_space_rows(df)\n",
    "    df.to_csv(concatenated_path, index=False)\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total time for processing: {total_time:.2f} seconds\")\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copy = dataset.copy() # For visual purposes\n",
    "dataset = preprocess_dataset(dataset)\n",
    "\n",
    "b_length = len(dataset_copy)\n",
    "a_length = len(dataset)\n",
    "b_mean_length = np.mean(dataset_copy['Message'].str.len())\n",
    "a_mean_length = np.mean(dataset['Message'].str.len())\n",
    "b_max_length = np.max(dataset_copy['Message'].str.len())\n",
    "a_max_length = np.max(dataset['Message'].str.len())\n",
    "longest_sentence_index = dataset['Message'].str.len().idxmax()\n",
    "longest_sentence = dataset['Message'].iloc[longest_sentence_index]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Changes (Before/After) processing:\")\n",
    "print(f\"Length: {b_length} -> {a_length}\")\n",
    "print(f\"Median length: {b_mean_length:.2f} -> {a_mean_length:.2f}\")\n",
    "print(f\"Max sentence length: {b_max_length} -> {a_max_length}\")\n",
    "print(f\"Nan values: {dataset_copy.isna().sum().sum()} -> {dataset.isna().sum().sum()}\")\n",
    "print(f\"Longest sentence: {len(longest_sentence)} chars: {longest_sentence}\")\n",
    "\n",
    "del dataset_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving into Question / Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sort_values(by=['Date']).reset_index(drop=True)\n",
    "\n",
    "dataset['Date'] = pd.to_datetime(dataset['Date'], format='ISO8601')\n",
    "\n",
    "reference_time = dataset['Date'].min()\n",
    "dataset['time_diff_seconds'] = dataset['Date'] - reference_time\n",
    "# Converts into hours difference\n",
    "dataset['time_diff_seconds'] = dataset['time_diff_seconds'].apply(lambda x: int(x.total_seconds()))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sentences(df: pd.DataFrame) -> pd.DataFrame:\n",
    "      \"\"\"\n",
    "      Takes a pandas dataframe with a messages column and returns separated rows with question / answer columns\n",
    "      Args: \n",
    "            dataset: pd.DataFrame\n",
    "            Dataset should contain a messages column and first row with identification who sent a message.\n",
    "\n",
    "\n",
    "\n",
    "      Returns:\n",
    "            dataset: pd.DataFrame\n",
    "            \n",
    "            Dataset divided into question / answer columns.\n",
    "      \"\"\"\n",
    "\n",
    "      separated_dataset = pd.DataFrame(columns=['question', 'answer', 'timestamp', 'Sent_by_me', 'time_diff_seconds'])\n",
    "\n",
    "      if df[\"Sent_by_me\"].iloc[0]: \n",
    "            df = df.drop(df.index[0]).reset_index(drop=True)\n",
    "\n",
    "      questions_df = df[df.index % 2 == 0].reset_index(drop=True)\n",
    "      answers_df = df[df.index % 2 == 1].reset_index(drop=True)\n",
    "\n",
    "      min_length = min(len(questions_df), len(answers_df))\n",
    "      questions_df = questions_df[\"Message\"][:min_length]\n",
    "      answers_df = answers_df[\"Message\"][:min_length]\n",
    "      timestamp = df[\"Date\"][:min_length]\n",
    "      sent_by_me = df[\"Sent_by_me\"][:min_length]\n",
    "      time_diff_seconds = df[\"time_diff_seconds\"][:min_length]\n",
    "\n",
    "      separated_dataset[\"question\"] = questions_df\n",
    "      separated_dataset[\"answer\"] = answers_df\n",
    "      separated_dataset[\"timestamp\"] = timestamp\n",
    "      separated_dataset[\"Sent_by_me\"] = sent_by_me\n",
    "      separated_dataset[\"time_diff_seconds\"] = time_diff_seconds\n",
    "\n",
    "      return separated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_dataset = separate_sentences(dataset)\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding column with previous context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_context(df: pd.DataFrame, context_size: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a column with previous context to the DataFrame.\n",
    "    \n",
    "    The context is based on the previous messages. If the time difference \n",
    "    between messages is more than 2 hours, it's considered the start of a \n",
    "    new conversation, and the first row of that new conversation will have \n",
    "    no context. Subsequent messages in the conversation will have context.\n",
    "    \"\"\"\n",
    "    \n",
    "    context_list = []\n",
    "    last_time = None  # Track the last message time to determine time gaps\n",
    "    \n",
    "    for index in range(len(df)):\n",
    "        if index == 0:\n",
    "            # No context for the very first message\n",
    "            context_list.append(None)\n",
    "            last_time = df.loc[index, \"time_diff_seconds\"]\n",
    "            continue\n",
    "        \n",
    "        # Calculate the time difference from the previous row\n",
    "        time_diff = df.loc[index, \"time_diff_seconds\"] - last_time\n",
    "        last_time = df.loc[index, \"time_diff_seconds\"]\n",
    "\n",
    "        # If time_diff is more than 6 hours, consider it a new conversation\n",
    "        if time_diff > 21600:\n",
    "            context_list.append(None)  # Start of a new conversation, no context\n",
    "        else:\n",
    "            # Create context from the previous messages within the context size\n",
    "            start_index = max(index - context_size, 0)\n",
    "            context = df.loc[start_index:index - 1, [\"question\", \"answer\"]]\n",
    "\n",
    "            # Build the context string from previous rows\n",
    "            message = []\n",
    "            for key, (question, answer) in enumerate(zip(context[\"question\"], context[\"answer\"])):\n",
    "                message.append(f\"Q{key + 1}: {question}. A{key + 1}: {answer} || \")\n",
    "\n",
    "            # Append the concatenated message as the context\n",
    "            context_list.append(\" \".join(message))\n",
    "\n",
    "    # Handle 1st row None (diff seconds in 0 index is 0, then 1 is None).\n",
    "    context = df.loc[0, [\"question\", \"answer\"]]\n",
    "    question, answer = context[\"question\"], context[\"answer\"]\n",
    "    context_list[1] = (f\"Q{1}: {question}. A{1}: {answer} || \")\n",
    "    \n",
    "    # Add the context as a new column\n",
    "    df[\"context\"] = context_list\n",
    "\n",
    "    # Replace any empty or missing contexts with \"Missing Context\" if desired\n",
    "    df[\"context\"] = df[\"context\"].apply(lambda x: \"Time Gap\" if pd.isna(x) else x)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_dataset = add_context(separated_dataset)\n",
    "separated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time_gaps = (separated_dataset[\"context\"] == \"Time Gap\").sum()\n",
    "total_time_gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation \n",
    "Steps: \n",
    "1. On context | response dataset, process only responses and get the array of used words and how often they were used. \n",
    "2. Look up for dictionaries that contain synonyms to most used word.\n",
    "3. Go through all responses and think about the algorithm that would be able to efficiently augment the responses. \n",
    "\n",
    "Methods: \n",
    "1. Back-translation\n",
    "2. Synonym replacement\n",
    "3. Word Swap\n",
    "4. Sentence shuffle\n",
    "\n",
    "Remember that this notebook is designed for to work with ukrainian language dataset, and not all techniques will work for English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonyms using word embeddings (word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n",
    "import os \n",
    "for file in nltk.data.path:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample paragraph of text\n",
    "text = 'I love this flavor! It\\'s by far the best choice and my go-to whenever I go to the grocery store. I wish they would restock it more often though.'\n",
    "#Tokenize by word\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', download_dir='/Users/bohdan/nltk_data')  # Download to the custom path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"since it is for business\"]\n",
    "\n",
    "# Define a tokenizer function\n",
    "def tokenize_ukrainian(text):\n",
    "    tokens = nltk.word_tokenize(text, language='english')\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('ukrainian'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_ukrainian(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = separated_dataset.copy()\n",
    "dataset.apply(lambda x: tokenize_ukrainian(dataset[\"question\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
