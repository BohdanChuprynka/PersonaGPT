{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is for analyzing the steps during processing data. It contains a lot more documentation and code than the original script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_filter = os.getenv('KEYS_TO_FILTER').split(',')\n",
    "concatenated_path = os.getenv('CONCATENATED_PATH')\n",
    "dataset_path = \"Datasets/concatenated.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(dataset_path)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "      return re.sub(r'http\\S+', 'redacted', text)\n",
    "def remove_english_words(text):\n",
    "    # Looks for all English words and removes them.\n",
    "    pattern = r'\\b[a-zA-Z]+\\b'\n",
    "    return re.sub(pattern, '', text)\n",
    "def delete_html_tags(text):\n",
    "    clean_text = re.sub(r'<.*?>', '', text)\n",
    "    return clean_text\n",
    "def remove_mention(text):\n",
    "  mention_regex = r\"@\\w+\"\n",
    "  return re.sub(mention_regex, \"/mention\", text)\n",
    "def redact_email(text): \n",
    "    return re.sub(r'\\S+@\\S+', '/email', text)\n",
    "def remove_password(text): \n",
    "    copy_text = text\n",
    "    pass_pattern = r'[A-Za-z0-9@#$%^&+=]{8,}'\n",
    "    text_ = re.sub(pass_pattern, '', text)\n",
    "    return text_\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, ' ', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sensitive_words(sentence, replacement='CENSORED'):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        replacement: str = words that will be substituted instead of the sensitive words   \n",
    "    \"\"\"\n",
    "    words = set(keys_to_filter)\n",
    "    sentence_words = sentence.split()\n",
    "    \n",
    "    modified_sentence = [\n",
    "        replacement if word in words else word for word in sentence_words\n",
    "    ]\n",
    "    \n",
    "    # Join the list back into a sentence\n",
    "    return ' '.join(modified_sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "      text = remove_english_words(text)\n",
    "      text = remove_password(text)\n",
    "      text = redact_email(text)\n",
    "      text = remove_urls(text)\n",
    "      text = remove_mention(text)\n",
    "      text = delete_html_tags(text)\n",
    "      text = filter_sensitive_words(text)\n",
    "      text = remove_whitespace(text)\n",
    "      \n",
    "      return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    import time \n",
    "    dataset_copy = df.copy()\n",
    "    start_time= time.time()\n",
    "    df['Message'] = df['Message'].apply(preprocess_data)\n",
    "    df[\"Message\"] = df[\"Message\"].apply(lambda x: remove_emojis(str(x)) if isinstance(x, str) else ' ')\n",
    "    df.to_csv(concatenated_path, index=False)\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total time for processing: {total_time:.2f} seconds\")\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copy = dataset.copy() # For visual purposes\n",
    "dataset = preprocess_dataset(dataset)\n",
    "\n",
    "b_length = len(dataset_copy)\n",
    "a_length = len(dataset)\n",
    "b_mid_length = np.mean(dataset_copy['Message'].str.len())\n",
    "a_mid_length = np.mean(dataset['Message'].str.len())\n",
    "b_max_length = np.max(dataset_copy['Message'].str.len())\n",
    "a_max_length = np.max(dataset['Message'].str.len())\n",
    "longest_sentence = max(dataset['Message'])\n",
    "\n",
    "\n",
    "print(f\"Changes (Before/After) processing:\")\n",
    "print(f\"Length: {b_length} -> {a_length}\")\n",
    "print(f\"Median length: {b_mid_length:.2f} -> {a_mid_length:.2f}\")\n",
    "print(f\"Max sentence length: {b_max_length} -> {a_max_length}\")\n",
    "print(f\"Nan values: {dataset_copy.isna().sum().sum()} -> {dataset.isna().sum().sum()}\")\n",
    "print(f\"Longest sentence: {len(longest_sentence)} chars: {longest_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MAKE IT ALL WORK\n",
    "# LOOK AT OTHER NLP PROCESSING TOOLS YOU CAN USE FOR BEST ACCURACY \n",
    "# SEPARATE CONTEXT AND RESPONSE \n",
    "# GPT PROMPT OR RESEARCH TOPIC: WHAT ARE THE PROCESSING ALGORTIHMS DATA SCIENTISTS USED FOR PROCESSING DATASET FOR CHATGPT.\n",
    "# LOOK AT THE DATASET AND ANALYZE IT \n",
    "# AFTER DONE WITH PROCESSING, GO TO FIND BEST LLM TO TRAIN ON YOUR DATASET \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
