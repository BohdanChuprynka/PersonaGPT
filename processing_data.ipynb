{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is for analyzing the steps during processing data. It contains a lot more documentation and code than the original script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_filter = os.getenv('KEYS_TO_FILTER').split(',')\n",
    "concatenated_path = os.getenv('CONCATENATED_PATH')\n",
    "dataset_path = \"Datasets/concatenated.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(dataset_path)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "      return re.sub(r'http\\S+', 'redacted', text)\n",
    "def remove_english_words(text):\n",
    "    # Looks for all English words and removes them.\n",
    "    pattern = r'\\b[a-zA-Z]+\\b'\n",
    "    return re.sub(pattern, '', text)\n",
    "def delete_html_tags(text):\n",
    "    clean_text = re.sub(r'<.*?>', '', text)\n",
    "    return clean_text\n",
    "def remove_mention(text):\n",
    "  mention_regex = r\"@\\w+\"\n",
    "  return re.sub(mention_regex, \"/mention\", text)\n",
    "def redact_email(text): \n",
    "    return re.sub(r'\\S+@\\S+', '/email', text)\n",
    "def remove_password(text): \n",
    "    copy_text = text\n",
    "    pass_pattern = r'[A-Za-z0-9@#$%^&+=]{8,}'\n",
    "    text_ = re.sub(pass_pattern, '', text)\n",
    "    return text_\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, ' ', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sensitive_words(sentence, replacement='CENSORED'):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        replacement: str = words that will be substituted instead of the sensitive words   \n",
    "    \"\"\"\n",
    "    words = set(keys_to_filter)\n",
    "    sentence_words = sentence.split()\n",
    "    \n",
    "    modified_sentence = [\n",
    "        replacement if word in words else word for word in sentence_words\n",
    "    ]\n",
    "    \n",
    "    # Join the list back into a sentence\n",
    "    return ' '.join(modified_sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "      text = remove_english_words(text)\n",
    "      text = remove_password(text)\n",
    "      text = redact_email(text)\n",
    "      text = remove_urls(text)\n",
    "      text = remove_mention(text)\n",
    "      text = delete_html_tags(text)\n",
    "      text = filter_sensitive_words(text)\n",
    "      text = remove_whitespace(text)\n",
    "      \n",
    "      return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    import time \n",
    "    dataset_copy = df.copy()\n",
    "    start_time= time.time()\n",
    "    df['Message'] = df['Message'].apply(preprocess_data)\n",
    "    df[\"Message\"] = df[\"Message\"].apply(lambda x: remove_emojis(str(x)) if isinstance(x, str) else ' ')\n",
    "    df.to_csv(concatenated_path, index=False)\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total time for processing: {total_time:.2f} seconds\")\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copy = dataset.copy() # For visual purposes\n",
    "dataset = preprocess_dataset(dataset)\n",
    "\n",
    "b_length = len(dataset_copy)\n",
    "a_length = len(dataset)\n",
    "b_mid_length = np.mean(dataset_copy['Message'].str.len())\n",
    "a_mid_length = np.mean(dataset['Message'].str.len())\n",
    "b_max_length = np.max(dataset_copy['Message'].str.len())\n",
    "a_max_length = np.max(dataset['Message'].str.len())\n",
    "longest_sentence = max(dataset['Message'])\n",
    "\n",
    "\n",
    "print(f\"Changes (Before/After) processing:\")\n",
    "print(f\"Length: {b_length} -> {a_length}\")\n",
    "print(f\"Median length: {b_mid_length:.2f} -> {a_mid_length:.2f}\")\n",
    "print(f\"Max sentence length: {b_max_length} -> {a_max_length}\")\n",
    "print(f\"Nan values: {dataset_copy.isna().sum().sum()} -> {dataset.isna().sum().sum()}\")\n",
    "print(f\"Longest sentence: {len(longest_sentence)} chars: {longest_sentence}\")\n",
    "\n",
    "del dataset_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving into Question / Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sentences(df: pd.DataFrame) -> pd.DataFrame:\n",
    "      \"\"\"\n",
    "      Takes a pandas dataframe with a messages column and returns separated rows with question / answer columns\n",
    "      Args: \n",
    "            dataset: pd.DataFrame\n",
    "            Dataset should contain a messages column and first row with identification who sent a message.\n",
    "\n",
    "\n",
    "      Returns:\n",
    "            dataset: pd.DataFrame\n",
    "            \n",
    "            Dataset divided into question / answer columns.\n",
    "      \"\"\"\n",
    "\n",
    "      separated_dataset = pd.DataFrame(columns=['question', 'answer', \"timestamp\"])\n",
    "\n",
    "      if df[\"Sent_by_me\"].iloc[0]: \n",
    "            df = df.drop(df.index[0]).reset_index(drop=True)\n",
    "\n",
    "      questions_df = df[df.index % 2 == 0].reset_index(drop=True)\n",
    "      answers_df = df[df.index % 2 == 1].reset_index(drop=True)\n",
    "\n",
    "      min_length = min(len(questions_df), len(answers_df))\n",
    "      questions_df = questions_df[\"Message\"][:min_length]\n",
    "      answers_df = answers_df[\"Message\"][:min_length]\n",
    "      \n",
    "      separated_dataset[\"question\"] = questions_df\n",
    "      separated_dataset[\"answer\"] = answers_df\n",
    "      separated_dataset[\"timestamp\"] = df[\"Date\"][:min_length]\n",
    "\n",
    "      return separated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_dataset = separate_sentences(dataset)\n",
    "separated_dataset.head(100)\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding column with previous context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context(df: pd.DataFrame, context_size: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a column with previous context to the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    context_list = []\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        start_index = max(index - context_size, 0)\n",
    "        context = df.loc[start_index:index - 1, [\"question\", \"answer\"]]\n",
    "\n",
    "        message = []\n",
    "        for key, (question, answer) in enumerate(zip(context[\"question\"], context[\"answer\"])):\n",
    "            message.append(f\"Q{key + 1}: {question}. A{key + 1}: {answer}::\")\n",
    "\n",
    "        # Append the concatenated message to the context list\n",
    "        context_list.append(\" \".join(message))\n",
    "\n",
    "    # Add the context as a new column\n",
    "    df[\"context\"] = context_list\n",
    "\n",
    "    # Replace empty contexts with \"New Conversation\"\n",
    "    df[\"context\"] = df[\"context\"].apply(lambda x: \"Missing Context\" if len(x) == 0 else x)\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_dataset = add_context(separated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation \n",
    "Steps: \n",
    "1. On context | response dataset, process only responses and get the array of used words and how often they were used. \n",
    "2. Look up for dictionaries that contain synonyms to most used word.\n",
    "3. Go through all responses and think about the algorithm that would be able to efficiently augment the responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
