
loading_parameters:
  t_global_save_path: 'parsers/telegram/result.csv'  # If t_parse_type is "global", then fill it
  save_path: "Datasets"  # Datasets folder to save our datasets
  t_parse_type: "local"  # "local" or "global": Whether to parse your messages through local JSON files (Fast way) or globally via API (takes 1 hour for ~20k messages)
  telegram: true         # Whether to parse Telegram during data collection
  instagram: true        # Whether to parse Instagram data
  checkpoints: true      # Turn on if telegram parse is 'global'
  jailbreak_protection: false # Whether to use jailbreak protection (Only for UA language!)

  message_limit: null    # The maximum number of messages to be processed total (default: all)
  dialogs_limit: null    # The maximum number of dialogs to be processed
  verbose: true             # Whether to print the output
  threshold: 50          # Dialogs with less than this threshold of messages will be dropped



processing_parameters:
  dataset_path: 'Datasets/final_result.csv'
  save_path: 'Datasets/processed_dataset.csv'
  ua_path: "Datasets/stopwords_ua_set.txt"
  glove_path: 'Models/glove.6B.100d.txt'  # Synonym replacement model
  chunks_path: 'data_chunks'              # Path to the folder that contains the data chunks

  censor_word: "CENSORED"                 # The word that will be placed instead of filtered sensitive words in the filter_sensitive_words function
  context_size: 10                        # The amount of previous messages to include in the context column (20 by default)
  time_threshold: 21600                   # The maximum time difference between messages to consider them part of the same conversation
  num_chunks: 32                          # Number of chunks to split the dataset into (32 by default)
  dataset_language: "uk"                 # The native language of the dataset
  back_translation_language: "en"         # The language to translate to and back from (en by default)
  probs: [0.1, 0.3, 0.3, 0.3]             # Probabilities for back-translation, shuffle, pop, and swap
  bool_synonym: true                      # Whether to perform synonym replacement together with back translation
  synonym_percentage: 0.7                 # The percentage of words to replace with synonyms (70% default)
  random_augmentation: true               # Whether to use random augmentation on each sentence (True by default)

  # Parallel processing   
  num_workers: 10         # Number of cores to use in parallel computing
  memory_threshold: 2                     # Memory (GB) to leave available during augmentation (2 by default)
  swap_processing: true                   # Swap memory during augmentation (True by default)
  delay: 10                               # Seconds to wait before continuing augmentation if memory_threshold is reached (10 by default)
  init_time: 5                            # Initialization time for workers (5 by default)
  
processing_kwargs:    
  augmentation_factor: 5                  # How many times to augment each question (5 by default)
  random_augmentation: true               # Whether to use random augmentation or not (True by default)
  samples: null                           # Number of rows to process (None/All by default)


training_parameters:
  DATA_PATH: "Datasets/processed_dataset.csv" # Change to processed_dataset when finished 
  TRAIN_PATH: "Datasets/train_dataset.pt"
  VAL_PATH: "Datasets/val_dataset.pt"
  OUTPUT_DIR: "Models/PersonaGPT_Mistral"
  MODEL_NAME: "SherlockAssistant/Mistral-7B-Instruct-Ukrainian"
  TRAIN_SIZE: 0.9
  NUM_TRAIN_EPOCHS: 1
  PER_DEVICE_TRAIN_BATCH_SIZE: 2
  PER_DEVICE_EVAL_BATCH_SIZE: 2
  LEARNING_RATE: 0.0001
  SEED: 42
  EVAL_STEPS: 314
  SAVE_STEPS: 314
  LOGGING_STEPS: 314
  SAVE_TOTAL_LIMIT: 5
  TARGET_MAX_LENGTH: 128
  MAX_LENGTH: 512
  DATASET_LANGUAGE: "uk"
  # Change for your preferences 
  MODEL_PROMPT: "Ви – Богдан, і ваше завдання – відповідати як Богдан. Ви повинні відтворити його стиль письма, тон і шаблони на основі заданого контексту та запитання. Контекст надає останні 10 пар запитання-відповідь, і ви повинні використовувати його, щоб дізнатися про моделі та структуру відповідей Богдана. Враховуючи нове запитання, створіть відповідь, яка віддзеркалює його стиль розмови, зберігаючи потік природним і узгодженим із попередніми відповідями. Пам'ятайте, що ви копіюєте персону, яка ніколи не буде повідомляти жодної приватної інформації та адресувати негативні емоції в сторону іншої, знайомої людини."

