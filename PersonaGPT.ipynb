{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from config import training_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Initializing parameters from config...\")\n",
    "\n",
    "# Load config and assign all the keys to variables \n",
    "if training_parameters:\n",
    "    locals().update(training_parameters)\n",
    "else:\n",
    "    # Paths and Hyperparameters\n",
    "    warnings.warn(\"Custom parameters are not available. Using default values...\")\n",
    "    root_folder = os.path.abspath(os.getcwd())\n",
    "    DATA_PATH = os.path.join(root_folder, \"Datasets/final_result.csv\")  # Update this path\n",
    "    MODEL_NAME = 'gpt2-medium'  # You can choose 'gpt2', 'gpt2-medium', etc.\n",
    "    OUTPUT_DIR = os.path.join(root_folder, \"Models/1.0v_PersonaGPT\")\n",
    "    train_size = 0.6\n",
    "    MAX_LENGTH = 256\n",
    "    BATCH_SIZE = 24  \n",
    "    EPOCHS = 3\n",
    "    LEARNING_RATE = 5e-5\n",
    "    WARMUP_RATIO = 5 \n",
    "    SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class ConstructDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        input_text = self.construct_input(row)\n",
    "        encoded_dict = self.tokenizer(\n",
    "            input_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_dict['input_ids'].squeeze()\n",
    "        attention_mask = encoded_dict['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': input_ids.clone(),  # Language modeling objective\n",
    "        }\n",
    "\n",
    "    def construct_input(self, row):\n",
    "        context = row['context']\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        # Construct the input in a conversational format\n",
    "        input_text = f\"{context}\\nUser: {question}\\nAssistant: {answer}\"\n",
    "        return input_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointScheduler():\n",
    "      def __init__(self, model, optimizer, save_dir, monitor='val_loss', mode='min', save_best_only=True, save_freq=1):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.save_dir = save_dir\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_freq = save_freq\n",
    "        self.best_metric = None\n",
    "      \n",
    "      def save_checkpoint(self, epoch, val_loss=None, val_accuracy=None):\n",
    "            \"\"\"Saves the model and optimizer state.\"\"\"\n",
    "            state = {\n",
    "                  'epoch': epoch,\n",
    "                  'model_state_dict': self.model.state_dict(),\n",
    "                  'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                  'val_loss': val_loss,\n",
    "                  'val_accuracy': val_accuracy,\n",
    "            }\n",
    "            save_path = os.path.join(self.save_dir, f'checkpoint_epoch_{epoch}.pth')\n",
    "            torch.save(state, save_path)\n",
    "            print(f'Checkpoint saved at {save_path}')\n",
    "      \n",
    "      def step(self, epoch, val_loss=None, val_acc=None):\n",
    "          \"\"\"\n",
    "          Decision maker whether to save the model during training step.\n",
    "          \"\"\"\n",
    "          if epoch % self.save_freq == 0:\n",
    "              current_metric = val_loss if self.monitor == 'val_loss' else val_acc\n",
    "              if current_metric is None:\n",
    "                  return\n",
    "\n",
    "              if self.save_best_only:\n",
    "                 if self.best_metric is None:\n",
    "                     self.best_metric = current_metric\n",
    "                     self.save_checkpoint(epoch, val_loss, val_acc)\n",
    "                 else: \n",
    "                     improvement = (current_metric > self.best_metric) if self.mode == 'min' else (current_metric < self.best_metric)\n",
    "\n",
    "                     if improvement:\n",
    "                         self.best_metric = current_metric\n",
    "                         self.save_checkpoint(epoch, val_loss, val_acc)\n",
    "              else:\n",
    "                  self.save_checkpoint(epoch, val_loss, val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing Function\n",
    "def load_data(file_path):\n",
    "    logger.info(\"Loading dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "    train_idx = int(train_size * len(df))\n",
    "    df_train = df[:train_idx]\n",
    "    df_val = df[train_idx:]\n",
    "\n",
    "    return df_train, df_val\n",
    "\n",
    "# Collate Function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids.to(device),\n",
    "        'attention_mask': attention_mask.to(device),\n",
    "        'labels': labels.to(device),\n",
    "    }\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # Load and preprocess data\n",
    "    train_data, val_data = load_data(DATA_PATH)\n",
    "\n",
    "    # Initialize tokenizer and model\n",
    "    logger.info(\"Initializing tokenizer and model...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # GPT-2 does not have a padding token\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare dataset and dataloader\n",
    "    dataset = ConstructDataset(train_data, tokenizer, max_length=MAX_LENGTH)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Set up optimizer and scheduler\n",
    "    logger.info(\"Setting up optimizer and scheduler...\")\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
    "    total_steps = len(dataloader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=WARMUP_RATIO, num_training_steps=total_steps\n",
    "    )\n",
    "    checkpoint_scheduler = CheckpointScheduler(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    save_dir='./checkpoints',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    save_freq=1  # Save every epoch if there is an improvement\n",
    ")\n",
    "\n",
    "    # Training Loop # TODO: Add checkpoints\n",
    "    logger.info(\"Starting training...\")\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        logger.info(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "        for batch in progress_bar:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels'],\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                checkpoint_scheduler.step(epoch, val_loss=loss.item(), val_acc=None)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "                avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "                logger.info(f\"Average Epoch Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    logger.info(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    logger.info(\"Model saved successfully.\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "      main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
