{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgg8Z6tlt_fw",
    "outputId": "f5126b7d-bb4a-4b4d-e055-03a607772150"
   },
   "outputs": [],
   "source": [
    "pip install python-dotenv datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgxz_sYTbpXE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "#import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import MT5ForConditionalGeneration, MT5TokenizerFast\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EvalPrediction\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset\n",
    "import dataclasses\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7lyDYDrK8eY"
   },
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as f:\n",
    "    full_config = yaml.safe_load(f)\n",
    "\n",
    "training_params = full_config.get('training_parameters', {})\n",
    "\n",
    "ROOT_PATH = os.getcwd()\n",
    "DATA_PATH                   = os.path.join(ROOT_PATH, training_params.get(\"DATA_PATH\"))\n",
    "TRAIN_PATH                  = os.path.join(ROOT_PATH, training_params.get(\"TRAIN_PATH\"))\n",
    "VAL_PATH                    = os.path.join(ROOT_PATH, training_params.get(\"VAL_PATH\"))\n",
    "OUTPUT_DIR                  = os.path.join(ROOT_PATH, training_params.get(\"OUTPUT_DIR\", \"Models/PersonaGPT\"))\n",
    "SAVE_STRATEGY               = training_params.get(\"SAVE_STRATEGY\", \"epoch\")\n",
    "OVERWRITE_OUTPUT_DIR        = training_params.get(\"OVERWRITE_OUTPUT_DIR\", True)\n",
    "MODEL_NAME                  = training_params.get(\"MODEL_VERSION\", \"google/mt5-base\")\n",
    "TRAIN_SIZE                  = training_params.get(\"TRAIN_SIZE\", 0.9)\n",
    "# MAX_SEQ_LENGTH              = training_params.get(\"MAX_SEQ_LENGTH\", 256)\n",
    "TARGET_MAX_LENGTH           = training_params.get(\"TARGET_MAX_LENGTH\", 128)\n",
    "NUM_TRAIN_EPOCHS            = training_params.get(\"NUM_TRAIN_EPOCHS\", 3)\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = training_params.get(\"PER_DEVICE_TRAIN_BATCH_SIZE\", 8)\n",
    "PER_DEVICE_EVAL_BATCH_SIZE  = training_params.get(\"PER_DEVICE_EVAL_BATCH_SIZE\", 8)\n",
    "LEARNING_RATE               = training_params.get(\"LEARNING_RATE\", 5e-5)\n",
    "WARMUP_STEPS                = training_params.get(\"WARMUP_STEPS\", 500)\n",
    "SEED                        = training_params.get(\"SEED\", 42)\n",
    "FP16                        = training_params.get(\"FP16\", True)\n",
    "EVALUATION_STRATEGY         = training_params.get(\"EVALUATION_STRATEGY\", \"steps\")\n",
    "EVAL_STEPS                  = training_params.get(\"EVAL_STEPS\", 500)\n",
    "SAVE_STEPS                  = training_params.get(\"SAVE_STEPS\", 1000)\n",
    "LOGGING_STEPS               = training_params.get(\"LOGGING_STEPS\", 100)\n",
    "SAVE_TOTAL_LIMIT            = training_params.get(\"SAVE_TOTAL_LIMIT\", 2)\n",
    "MAX_LENGTH                  = training_params.get(\"MAX_LENGTH\", 128)\n",
    "NUM_BEAMS                   = training_params.get(\"NUM_BEAMS\", 5)\n",
    "DATASET_LANGUAGE            = training_params.get(\"DATASET_LANGUAGE\", \"en\")\n",
    "UK_PRONOUNCES               = ['ukrainian', 'ukraine', 'ua', 'ukr', 'uk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHj1pPcM0NYv"
   },
   "outputs": [],
   "source": [
    "# For google collab\n",
    "\n",
    "LOGGING_STEPS = 30\n",
    "EVAL_STEPS = 30\n",
    "WARMUP_STEPS = 30\n",
    "LOGGING_STEPS = 30\n",
    "SAVE_STEPS = 30\n",
    "SAVE_TOTAL_LIMIT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ssub86RroR1p"
   },
   "outputs": [],
   "source": [
    "# tokenizer.batch_encode_plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nonIaOWJ4war"
   },
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gwxBATGC6m4"
   },
   "outputs": [],
   "source": [
    "def change_prompts(df):\n",
    "    if UK_PRONOUNCES and DATASET_LANGUAGE.lower() in UK_PRONOUNCES:  # type: ignore\n",
    "        # Replace the \"Time Gap\" for ukrainian translation\n",
    "        df[\"Context\"] = [\"Відсутній контекст\" if x == \"Time Gap\" else x for x in df[\"Context\"]]\n",
    "        q_prompt = \"Питання\"\n",
    "        r_prompt = \"Відповідь\"\n",
    "        c_prompt = \"Контекст\"\n",
    "    else:\n",
    "        q_prompt = \"Question\"\n",
    "        r_prompt = \"Відповідь\"\n",
    "        c_prompt = \"Context\"\n",
    "\n",
    "    return df, q_prompt, r_prompt, c_prompt\n",
    "def load_data():\n",
    "  def preprocess_function(batch):\n",
    "    inputs = [f\"[{date}]\\n{c_prompt}: {context}\\n\\n{q_prompt}: {question}\\n{r_prompt}:\"\n",
    "            for date, context, question in zip(batch['timestamp'], batch['Context'], batch['question'])]\n",
    "    targets = batch['answer']\n",
    "\n",
    "    model_inputs = tokenizer(inputs, padding='max_length', truncation=True, max_length=256)\n",
    "    labels = tokenizer(targets, padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "    model_inputs = {key: model_inputs[key] for key in ['input_ids', 'attention_mask']}\n",
    "    model_inputs['labels'] = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels['input_ids']]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "  # Load the model\n",
    "  tokenizer = MT5TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "   # Load your CSV data\n",
    "  df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "  # For ukrainian only, changes the prompts for ukrainian version.\n",
    "  if DATASET_LANGUAGE == 'uk':\n",
    "    df, q_prompt, r_prompt, c_prompt = change_prompts(df)\n",
    "\n",
    "  # Create Hugging Face Dataset\n",
    "  dataset = Dataset.from_pandas(df)\n",
    "  dataset = dataset.train_test_split(train_size=TRAIN_SIZE)\n",
    "  train_dataset = dataset['train']\n",
    "  valid_dataset = dataset['test']\n",
    "\n",
    "  train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "  valid_dataset = valid_dataset.map(preprocess_function, batched=True, remove_columns=valid_dataset.column_names, load_from_cache_file=False)\n",
    "\n",
    "  # Save the dataset directly for training\n",
    "  torch.save(train_dataset, TRAIN_PATH)\n",
    "  torch.save(valid_dataset, VAL_PATH)\n",
    "  \n",
    "  return train_dataset, valid_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582,
     "referenced_widgets": [
      "def3692a2fbf46ed8e94813332241a1a",
      "adf64ded9b7a46a28bd5e09b2e28ad0c",
      "1563774a979d4a3b9cd19885dba0cdd2",
      "2f2fdf39299e48c0b9735f89b1a2101e",
      "30a42c9380dc43f7ab143c409a3db882",
      "0394364bf43941bca02e4d788c4cc13d",
      "31327b762c1149c6890a1455d5d42098",
      "633a52558da34c7bba77fc5faa1ae1d5",
      "6530af83708b4a1aa8318438975e7a85",
      "e2e0727049fd4b7c915833006b4aa8a8",
      "60c1be3844eb4e7183768ee3efcb09d6"
     ]
    },
    "id": "ByTjNzMG4was",
    "outputId": "7bddb42f-a014-4688-cfc0-512b76a0980e"
   },
   "outputs": [],
   "source": [
    "training_dataset, valid_dataset = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ar5D4hfFzLBi"
   },
   "source": [
    "# Setting up the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wR1ESOPLymA3"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "    train_file_path: Optional[str] = field(\n",
    "        default=TRAIN_PATH if TRAIN_PATH else 'Datasets/train_dataset.pt', # TODO: remove space\n",
    "        metadata={\"help\": \"Path for cached train dataset\"},\n",
    "    )\n",
    "    valid_file_path: Optional[str] = field(\n",
    "        default=VAL_PATH if VAL_PATH else 'Datasets/val_dataset.pt',\n",
    "        metadata={\"help\": \"Path for cached valid dataset\"},\n",
    "    )\n",
    "    max_len: Optional[int] = field(\n",
    "        default=MAX_LENGTH ,\n",
    "        metadata={\"help\": \"Max input length for the source text\"},\n",
    "    )\n",
    "    target_max_len: Optional[int] = field(\n",
    "        default=TARGET_MAX_LENGTH,\n",
    "        metadata={\"help\": \"Max input length for the target text\"},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OpeRUPl4was"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFZTTu933zwj"
   },
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def main():\n",
    "  parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))\n",
    "\n",
    "  model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath('args.json'))\n",
    "\n",
    "  training_args = Seq2SeqTrainingArguments(\n",
    "      output_dir=OUTPUT_DIR,\n",
    "      save_strategy=SAVE_STRATEGY,\n",
    "      overwrite_output_dir=OVERWRITE_OUTPUT_DIR,\n",
    "      per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "      per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "      gradient_accumulation_steps=4,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "      warmup_steps=WARMUP_STEPS,\n",
    "      fp16=FP16,\n",
    "      evaluation_strategy=EVALUATION_STRATEGY,\n",
    "      eval_steps=EVAL_STEPS,\n",
    "      save_steps=SAVE_STEPS,\n",
    "      logging_steps=LOGGING_STEPS,\n",
    "      save_total_limit=1,\n",
    "      max_grad_norm=1.0\n",
    "      do_train=True\n",
    "  )\n",
    "\n",
    "\n",
    "  # Set up logging\n",
    "  logging.basicConfig(\n",
    "      format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "      datefmt='%Y-%m-%d %H:%M:%S',\n",
    "      level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARNING,\n",
    "  )\n",
    "  logger = logging.getLogger(__name__)\n",
    "\n",
    "  # If saved model && ouput already exists\n",
    "  if (\n",
    "      os.path.exists(training_args.output_dir)\n",
    "      and os.listdir(training_args.output_dir)\n",
    "      and training_args.do_train\n",
    "      and not training_args.overwrite_output_dir\n",
    "  ):\n",
    "      raise ValueError(\n",
    "          f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome or change the output directory.\"\n",
    "      )\n",
    "  # Check for GPU availability\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  logger.info(f\"Using device: {device}\")\n",
    "  set_seed(SEED)\n",
    "\n",
    "  # Load the model\n",
    "  tokenizer = MT5TokenizerFast.from_pretrained(model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path)\n",
    "  model = MT5ForConditionalGeneration.from_pretrained(model_args.model_name_or_path,\n",
    "                                                      cache_dir=model_args.cache_dir if model_args.cache_dir else None)\n",
    "\n",
    "  # Load the dataset\n",
    "  logger.info(f\"Loading datasets...\")\n",
    "  if not os.path.exists(TRAIN_PATH) and not os.path.exists(VAL_PATH):\n",
    "    logger.info(f\"Loading datasets...\")\n",
    "    load_data()\n",
    "\n",
    "  train_dataset = torch.load(data_args.train_file_path)\n",
    "  valid_dataset = torch.load(data_args.valid_file_path)\n",
    "  data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,\n",
    "                                        model=model,\n",
    "                                        padding=True)\n",
    "  trainer = Seq2SeqTrainer(\n",
    "      model=model,\n",
    "      args=training_args,\n",
    "      train_dataset=train_dataset,\n",
    "      eval_dataset=valid_dataset,\n",
    "      data_collator=data_collator\n",
    "  )\n",
    "\n",
    "  # Training\n",
    "  if training_args.do_train:\n",
    "    # Check for available checkpoints; pick the latest one\n",
    "    if os.path.exists(training_args.output_dir) and os.listdir(training_args.output_dir) and training_args.resume_from_checkpoint is None:\n",
    "      logger.info(\"*** Starting from a checkpoint ***\")\n",
    "      checkpoint_dirs = sorted(\n",
    "          glob.glob(os.path.join(training_args.output_dir, \"checkpoint-*\")),\n",
    "          key=os.path.getmtime,\n",
    "          reverse=True\n",
    "      )\n",
    "      if checkpoint_dirs:\n",
    "          training_args.resume_from_checkpoint = checkpoint_dirs[0]\n",
    "\n",
    "    logger.info(\"*** Train ***\")\n",
    "    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
    "    trainer.save_model()\n",
    "\n",
    "  # Evaluation\n",
    "  results = {}\n",
    "  if training_args.do_eval and training_args.local_rank in [-1, 0]:\n",
    "      logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "      eval_output = trainer.evaluate()\n",
    "\n",
    "      output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "      with open(output_eval_file, \"w\") as writer:\n",
    "          logger.info(\"***** Eval results *****\")\n",
    "          for key in sorted(eval_output.keys()):\n",
    "              logger.info(\"  %s = %s\", key, str(eval_output[key]))\n",
    "              writer.write(\"%s = %s\\n\" % (key, str(eval_output[key])))\n",
    "\n",
    "      results.update(eval_output)\n",
    "\n",
    "  return results\n",
    "\n",
    "def _mp_fn(index):\n",
    "    # For xla_spawn (TPUs)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qblWQoMAMnu"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "args_dict = {\n",
    "  \"output_dir\": OUTPUT_DIR,\n",
    "  \"model_name_or_path\": MODEL_NAME,\n",
    "  \"max_len\": MAX_LENGTH ,\n",
    "  \"target_max_len\": TARGET_MAX_LENGTH,\n",
    "  \"save_strategy\": SAVE_STRATEGY,\n",
    "  \"overwrite_output_dir\": OVERWRITE_OUTPUT_DIR,\n",
    "  \"per_device_train_batch_size\": PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "  \"per_device_eval_batch_size\": PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "  \"gradient_accumulation_steps\": 4,\n",
    "  \"learning_rate\": LEARNING_RATE,\n",
    "  \"num_train_epochs\": NUM_TRAIN_EPOCHS,\n",
    "  \"do_train\": True,\n",
    "  \"warmup_steps\": WARMUP_STEPS,\n",
    "  \"fp16\": FP16,\n",
    "  \"evaluation_strategy\": EVALUATION_STRATEGY,\n",
    "  \"eval_steps\": EVAL_STEPS,\n",
    "  \"save_steps\": SAVE_STEPS,\n",
    "  \"logging_steps\": LOGGING_STEPS,\n",
    "  \"save_total_limit\": 1\n",
    "}\n",
    "\n",
    "with open('args.json', 'w') as f:\n",
    "  json.dump(args_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sW4OK4haFVZc",
    "outputId": "3625e385-685c-4740-d85f-1fdd8e4996ff"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TqlgY7wASqZ"
   },
   "outputs": [],
   "source": [
    "#xmp.spawn(_mp_fn, args=(), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtoY_q3bt6V0"
   },
   "source": [
    "\t•\tMetrics to consider:\n",
    "\t•\tBLEU: For text similarity.\n",
    "\t•\tROUGE: For overlap of phrases.\n",
    "\t•\tPerplexity: For model confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NeBcrZQt6V0"
   },
   "source": [
    "## Generating using MT5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0c3tTU_t6V0"
   },
   "outputs": [],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "# Specify the path to your saved model directory\n",
    "model_path = 'Models/1.1v_PersonaGPT'  # Replace with your actual path if different\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_path)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzBkcS6it6V0"
   },
   "outputs": [],
   "source": [
    "def generate(model, question, context, max_length=200, kwargs=None):\n",
    "      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "      model.to(device)\n",
    "\n",
    "      input_text = f\"{q_prompt}: {question} {c_prompt}: {context}\" if pd.notnull(context) else f\"{q_prompt}: {question}\"\n",
    "      input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "      input_ids = input_ids.to(device)\n",
    "      with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                  input_ids=input_ids,\n",
    "                  max_length=MAX_LENGTH,           # Maximum length of the generated answer\n",
    "                  num_beams=NUM_BEAMS,             # Beam search for better results\n",
    "                  early_stopping=True\n",
    "            )\n",
    "      # Decode the generated IDs to text\n",
    "      answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "      # Print the answer\n",
    "      print(\"Answer:\", answer)\n",
    "\n",
    "question = \"що ти там?\"\n",
    "context = \"Time Gap\"\n",
    "generate(model, question, context)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0394364bf43941bca02e4d788c4cc13d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1563774a979d4a3b9cd19885dba0cdd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_633a52558da34c7bba77fc5faa1ae1d5",
      "max": 105219,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6530af83708b4a1aa8318438975e7a85",
      "value": 21000
     }
    },
    "2f2fdf39299e48c0b9735f89b1a2101e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2e0727049fd4b7c915833006b4aa8a8",
      "placeholder": "​",
      "style": "IPY_MODEL_60c1be3844eb4e7183768ee3efcb09d6",
      "value": " 21000/105219 [00:14&lt;00:55, 1527.59 examples/s]"
     }
    },
    "30a42c9380dc43f7ab143c409a3db882": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31327b762c1149c6890a1455d5d42098": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60c1be3844eb4e7183768ee3efcb09d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "633a52558da34c7bba77fc5faa1ae1d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6530af83708b4a1aa8318438975e7a85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "adf64ded9b7a46a28bd5e09b2e28ad0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0394364bf43941bca02e4d788c4cc13d",
      "placeholder": "​",
      "style": "IPY_MODEL_31327b762c1149c6890a1455d5d42098",
      "value": "Map:  20%"
     }
    },
    "def3692a2fbf46ed8e94813332241a1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_adf64ded9b7a46a28bd5e09b2e28ad0c",
       "IPY_MODEL_1563774a979d4a3b9cd19885dba0cdd2",
       "IPY_MODEL_2f2fdf39299e48c0b9735f89b1a2101e"
      ],
      "layout": "IPY_MODEL_30a42c9380dc43f7ab143c409a3db882"
     }
    },
    "e2e0727049fd4b7c915833006b4aa8a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
